{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD v2 Processing & Caching Demo\n",
    "\n",
    "This notebook demonstrates the end-to-end data pipeline used by MemXLNet-QA:\n",
    "\n",
    "1. Process a (small) subset of SQuAD v2 into segment features and cache them.\n",
    "2. Re-run processing to show near-instant cache reuse.\n",
    "3. Inspect cached feature structure.\n",
    "4. Manually re-shard features into multiple chunks and stream them chunk-by-chunk (simulating lowâ€‘memory mode).\n",
    "5. Re-create a `SquadLikeQADataset` from cache and verify integrity.\n",
    "6. Build a time-step-major dataloader and inspect the per-time-step batches (showing how segments align across documents for memory propagation).\n",
    "\n",
    "We'll keep parameters small for speed. Adjust `max_examples`, `max_seq_length`, and `doc_stride` for fuller runs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tupa7/private/paper-revise/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== First processing (build cache) ===\n",
      "Processed 60 features in 0.06s\n",
      "\n",
      "=== Second processing (cache hit) ===\n",
      "Cache hit features: 60 (took 0.051s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'first_run_s': 0.05879807472229004,\n",
       " 'second_run_s': 0.05122876167297363,\n",
       " 'features': 60}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from transformers import XLNetTokenizerFast\n",
    "from src.data import (\n",
    "    process_and_cache_dataset,\n",
    "    create_dataset_from_cache,\n",
    "    ChunkedCacheManager,\n",
    "    create_dataloader,\n",
    ")\n",
    "\n",
    "# Configuration (tweak for larger experiments)\n",
    "dataset_name = \"squad_v2\"\n",
    "split = \"validation\"  # smaller than train; fine for demo\n",
    "max_examples = 60       # limit for speed\n",
    "max_seq_length = 384    # shorter than 384 default to speed up\n",
    "doc_stride = 64\n",
    "cache_dir = \"./.cache_demo\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Load base tokenizer (no memory tokens needed for pure data demo)\n",
    "tokenizer = XLNetTokenizerFast.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "print(\"=== First processing (build cache) ===\")\n",
    "start = time.time()\n",
    "feature_count = process_and_cache_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split=split,\n",
    "    cache_dir=cache_dir,\n",
    "    max_examples=max_examples,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    streaming_chunk_size=2000,   # single-chunk path in current helper\n",
    "    max_memory_gb=16.0,\n",
    "    use_streaming=False,\n",
    "    tokenizer=tokenizer,\n",
    "    max_n_segs=None,\n",
    ")\n",
    "first_duration = time.time() - start\n",
    "print(f\"Processed {feature_count} features in {first_duration:.2f}s\")\n",
    "\n",
    "print(\"\\n=== Second processing (cache hit) ===\")\n",
    "start = time.time()\n",
    "feature_count_cached = process_and_cache_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split=split,\n",
    "    cache_dir=cache_dir,\n",
    "    max_examples=max_examples,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    streaming_chunk_size=2000,\n",
    "    max_memory_gb=16.0,\n",
    "    use_streaming=False,\n",
    "    tokenizer=tokenizer,\n",
    "    max_n_segs=None,\n",
    ")\n",
    "second_duration = time.time() - start\n",
    "print(f\"Cache hit features: {feature_count_cached} (took {second_duration:.3f}s)\")\n",
    "\n",
    "cache_timing = {\"first_run_s\": first_duration, \"second_run_s\": second_duration, \"features\": feature_count}\n",
    "cache_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cache Directory Contents ===\n",
      "squad_v2_validation_chunk_0.cache                     910.0 KB\n",
      "multi_chunks                                            0.2 KB\n",
      "Loaded chunk_0 with 60 feature dicts\n",
      "Feature keys: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'example_id', 'segment_index', 'total_segments', 'offset_mapping', 'context']\n",
      "Distinct documents: 60\n",
      "Segments per doc (first 10): [('doc_0', 1), ('doc_1', 1), ('doc_2', 1), ('doc_3', 1), ('doc_4', 1), ('doc_5', 1), ('doc_6', 1), ('doc_7', 1), ('doc_8', 1), ('doc_9', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect cache directory contents and sample feature\n",
    "print(\"=== Cache Directory Contents ===\")\n",
    "for fname in os.listdir(cache_dir):\n",
    "    fpath = os.path.join(cache_dir, fname)\n",
    "    size_kb = os.path.getsize(fpath) / 1024\n",
    "    print(f\"{fname:50s} {size_kb:8.1f} KB\")\n",
    "\n",
    "# Load raw cached features (single chunk path produced by process_and_cache_dataset)\n",
    "cache_manager = ChunkedCacheManager(cache_dir, chunk_size=2000)\n",
    "modified_dataset_name = dataset_name  # no memory tokens used, so no suffix\n",
    "chunk_0 = cache_manager.load_chunk(modified_dataset_name, split, 0)\n",
    "print(f\"Loaded chunk_0 with {len(chunk_0)} feature dicts\")\n",
    "\n",
    "# Show keys of first feature\n",
    "first_feature_keys = list(chunk_0[0].keys()) if chunk_0 else []\n",
    "print(\"Feature keys:\", first_feature_keys)\n",
    "\n",
    "# Count distinct documents\n",
    "doc_ids = {f['example_id'] for f in chunk_0}\n",
    "print(f\"Distinct documents: {len(doc_ids)}\")\n",
    "\n",
    "# Distribution of segments per doc\n",
    "from collections import Counter\n",
    "seg_counts = Counter()\n",
    "for f in chunk_0:\n",
    "    seg_counts[f['example_id']] += 1\n",
    "print(\"Segments per doc (first 10):\", list(seg_counts.items())[:10])\n",
    "\n",
    "feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked cache already exists; skipping creation.\n",
      "Listing chunked shards:\n",
      "   squad_v2_validation_chunk_0.cache\n",
      "   squad_v2_validation_chunk_1.cache\n",
      "   squad_v2_validation_chunk_2.cache\n",
      "Shard sizes: [25, 25, 10]\n",
      "Total streamed features: 60\n",
      "Shard sizes: [25, 25, 10]\n",
      "Total streamed features: 60\n"
     ]
    }
   ],
   "source": [
    "# Manual multi-chunk creation (simulated streaming scenario)\n",
    "# We create a new directory with multiple shards of the same features for demonstration.\n",
    "chunked_cache_dir = os.path.join(cache_dir, \"multi_chunks\")\n",
    "os.makedirs(chunked_cache_dir, exist_ok=True)\n",
    "chunk_size = 25  # small to force several chunks\n",
    "chunked_cache_manager = ChunkedCacheManager(chunked_cache_dir, chunk_size=chunk_size)\n",
    "\n",
    "# Only shard if not already present\n",
    "if not chunked_cache_manager.cache_exists(dataset_name, split):\n",
    "    for i in range(0, len(chunk_0), chunk_size):\n",
    "        shard = chunk_0[i:i+chunk_size]\n",
    "        chunk_id = i // chunk_size\n",
    "        chunked_cache_manager.save_chunk(shard, dataset_name, split, chunk_id)\n",
    "    print(f\"Created {chunked_cache_manager.get_total_chunks(dataset_name, split)} chunk files in {chunked_cache_dir}\")\n",
    "else:\n",
    "    print(\"Chunked cache already exists; skipping creation.\")\n",
    "\n",
    "print(\"Listing chunked shards:\")\n",
    "for fname in sorted(os.listdir(chunked_cache_dir)):\n",
    "    print(\"  \", fname)\n",
    "\n",
    "# Simple streaming generator\n",
    "def stream_features(dataset_name: str, split: str, manager: ChunkedCacheManager):\n",
    "    total = manager.get_total_chunks(dataset_name, split)\n",
    "    for cid in range(total):\n",
    "        yield cid, manager.load_chunk(dataset_name, split, cid)\n",
    "\n",
    "stream_counts = []\n",
    "for cid, shard in stream_features(dataset_name, split, chunked_cache_manager):\n",
    "    stream_counts.append(len(shard))\n",
    "print(\"Shard sizes:\", stream_counts)\n",
    "print(\"Total streamed features:\", sum(stream_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[create_dataset_from_cache] Checking cache for 'squad_v2' (validation) in .cache_demo ...\n",
      "[create_dataset_from_cache] Cache hit: 1 chunk(s) detected. Loading ...\n",
      "[create_dataset_from_cache] Reconstructed dataset with 60 features across 60 documents (cache).\n",
      "Dataset length: 60 (should match feature_count: 60)\n",
      "Sample feature keys: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'example_id', 'segment_index', 'total_segments', 'offset_mapping', 'context']\n",
      "Segment indices: seg_index= 0 of 1\n",
      "Decoded answer tokens: France\n",
      "Context excerpt: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\" ...\n",
      "[create_dataset_from_cache] Reconstructed dataset with 60 features across 60 documents (cache).\n",
      "Dataset length: 60 (should match feature_count: 60)\n",
      "Sample feature keys: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'example_id', 'segment_index', 'total_segments', 'offset_mapping', 'context']\n",
      "Segment indices: seg_index= 0 of 1\n",
      "Decoded answer tokens: France\n",
      "Context excerpt: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\" ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tupa7/private/paper-revise/notebooks/../src/data.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[key] = torch.tensor(feature[key])\n"
     ]
    }
   ],
   "source": [
    "# Re-create dataset via high-level helper (fresh processing path internally)\n",
    "# Note: create_dataset_from_cache currently re-processes; for large runs you'd typically\n",
    "# rely on your own cache manager to avoid recompute. Here we use it to get a Dataset object.\n",
    "validation_dataset = create_dataset_from_cache(\n",
    "    dataset_name=dataset_name,\n",
    "    split=split,\n",
    "    cache_dir=cache_dir,\n",
    "    max_examples=max_examples,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    use_lazy_loading=False,\n",
    "    max_n_segs=None,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(f\"Dataset length: {len(validation_dataset)} (should match feature_count: {feature_count})\")\n",
    "\n",
    "# Sample feature & attempt to reconstruct answer text\n",
    "sample_feature = validation_dataset[0]\n",
    "input_ids = sample_feature['input_ids']\n",
    "start_pos = sample_feature['start_positions']\n",
    "end_pos = sample_feature['end_positions']\n",
    "# If CLS (likely no-answer) we mark it\n",
    "token_answer = None\n",
    "if start_pos != tokenizer.cls_token_id and end_pos != tokenizer.cls_token_id:\n",
    "    # naive slice; includes special tokens sometimes depending on segmentation; ok for demo\n",
    "    token_answer = tokenizer.decode(input_ids[start_pos:end_pos+1])\n",
    "\n",
    "print(\"Sample feature keys:\", list(sample_feature.keys()))\n",
    "print(\"Segment indices: seg_index=\", sample_feature['segment_index'], \"of\", sample_feature['total_segments'])\n",
    "print(\"Decoded answer tokens:\", token_answer)\n",
    "print(\"Context excerpt:\", sample_feature['context'][:200], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-step-major batches (document groups): 15\n",
      "Time steps in first group: 1\n",
      "T00: input_ids shape=(4, 384), active_docs=4, example_ids=['doc_0', 'doc_1', 'doc_2', 'doc_3']\n",
      "Padding entries present? False\n"
     ]
    }
   ],
   "source": [
    "# Time-step-major DataLoader demonstration\n",
    "# Build dataloader: groups documents; each yielded element is a list of per-time-step batches.\n",
    "timestep_dataloader = create_dataloader(\n",
    "    dataset=validation_dataset,\n",
    "    batch_size=4,  # number of documents processed in parallel\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    use_time_step_major=True,\n",
    ")\n",
    "\n",
    "print(f\"Time-step-major batches (document groups): {len(timestep_dataloader)}\")\n",
    "\n",
    "# Inspect the first group of time-step batches\n",
    "first_group = next(iter(timestep_dataloader))\n",
    "print(f\"Time steps in first group: {len(first_group)}\")\n",
    "\n",
    "for t, step_batch in enumerate(first_group[:5]):  # cap display to first 5 steps\n",
    "    ids = step_batch['example_ids']\n",
    "    mask = step_batch['document_mask']\n",
    "    input_shape = step_batch['input_ids'].shape\n",
    "    active_docs = mask.sum().item()\n",
    "    print(f\"T{t:02d}: input_ids shape={tuple(input_shape)}, active_docs={active_docs}, example_ids={ids}\")\n",
    "\n",
    "# Optional: demonstrate how padding entries appear when documents have fewer segments\n",
    "padding_present = any(any(str(e).startswith('padding_') for e in step_batch['example_ids']) for step_batch in first_group)\n",
    "print(\"Padding entries present?\", padding_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
