{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 Debug and Analysis Notebook\n",
    "\n",
    "This notebook contains debugging and analysis code for Phase 2 of the XLNet QA training with memory capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and basic setup\n",
    "from pathlib import Path\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "# Project paths\n",
    "ROOT = Path.cwd().parent\n",
    "BEST_MODEL_DIR = ROOT/\"outputs/xlnet-squad-phase2-1/stage_2_segs_2/best_model\"\n",
    "TRAINING_CONFIG_PATH = BEST_MODEL_DIR/\"training_config.json\"\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Best model dir: {BEST_MODEL_DIR}\")\n",
    "print(f\"Config path: {TRAINING_CONFIG_PATH}\")\n",
    "\n",
    "# Check if paths exist\n",
    "if BEST_MODEL_DIR.exists():\n",
    "    print(\"‚úÖ Best model directory found\")\n",
    "else:\n",
    "    print(\"‚ùå Best model directory not found - may need to run training first\")\n",
    "    \n",
    "if TRAINING_CONFIG_PATH.exists():\n",
    "    print(\"‚úÖ Training config found\")\n",
    "else:\n",
    "    print(\"‚ùå Training config not found - may need to run training first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved training config (used to mirror original eval) - if available\n",
    "saved_cfg = {}\n",
    "if TRAINING_CONFIG_PATH.exists():\n",
    "    with open(TRAINING_CONFIG_PATH) as f:\n",
    "        saved_cfg = json.load(f)\n",
    "        \n",
    "    # Show key settings likely affecting low scores\n",
    "    key_cfg = {\n",
    "        k: saved_cfg.get(k)\n",
    "        for k in [\n",
    "            \"model_name\",\"progressive_segments\",\"max_n_segs\",\"memory_num_tokens\",\"memory_update\",\"memory_impl\",\n",
    "            \"use_global_softmax\",\"use_any_positive_logic\",\"no_answer_threshold\",\"max_seq_length\",\"doc_stride\",\n",
    "            \"use_streaming\",\"use_lazy_loading\",\"train_batch_size\",\"eval_batch_size\"\n",
    "        ]\n",
    "    }\n",
    "    print(\"Loaded saved config (subset):\")\n",
    "    pprint(key_cfg)\n",
    "else:\n",
    "    print(\"No saved config found - using defaults for demo\")\n",
    "    saved_cfg = {\n",
    "        \"model_name\": \"xlnet-base-cased\",\n",
    "        \"max_seq_length\": 384,\n",
    "        \"doc_stride\": 64,\n",
    "        \"memory_num_tokens\": 8,\n",
    "        \"memory_update\": \"gated\",\n",
    "        \"memory_impl\": \"token\",\n",
    "        \"use_global_softmax\": True,\n",
    "        \"use_any_positive_logic\": True,\n",
    "        \"no_answer_threshold\": 1.5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure src is importable\n",
    "import sys\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from src.train import TrainingConfig, XLNetRecurrentTrainer\n",
    "\n",
    "print(\"‚úÖ Successfully imported training modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(overrides=None):\n",
    "    cfg = TrainingConfig(**{**saved_cfg})\n",
    "    # Always evaluate from the best_model folder we loaded (if it exists)\n",
    "    if BEST_MODEL_DIR.exists():\n",
    "        cfg.model_name = str(BEST_MODEL_DIR)\n",
    "    # Make evaluation fast and memory-friendly\n",
    "    cfg.max_eval_samples = overrides.get(\"max_eval_samples\", 200) if overrides else 200\n",
    "    cfg.eval_batch_size = overrides.get(\"eval_batch_size\", 8) if overrides else 8\n",
    "    cfg.use_lazy_loading = overrides.get(\"use_lazy_loading\", True) if overrides else True\n",
    "    cfg.use_streaming = overrides.get(\"use_streaming\", True) if overrides else True\n",
    "    cfg.streaming_chunk_size = overrides.get(\"streaming_chunk_size\", 1000) if overrides else 1000\n",
    "    # Respect overrides for analysis\n",
    "    if overrides:\n",
    "        for k, v in overrides.items():\n",
    "            setattr(cfg, k, v)\n",
    "    # Keep cache/output local\n",
    "    cfg.cache_dir = str(ROOT/\"cache\")\n",
    "    cfg.output_dir = str(ROOT/\"outputs/debug_eval_phase2\")\n",
    "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_scenario(title, overrides=None):\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"Scenario: {title}\")\n",
    "    print(\"=\"*90)\n",
    "    cfg = build_config(overrides or {})\n",
    "    # Print the knobs we're testing\n",
    "    print(\"Settings:\")\n",
    "    print({\n",
    "        k: getattr(cfg, k)\n",
    "        for k in [\n",
    "            \"max_n_segs\",\"use_global_softmax\",\"use_any_positive_logic\",\"no_answer_threshold\",\n",
    "            \"memory_num_tokens\",\"memory_update\",\"memory_impl\",\"max_eval_samples\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        trainer = XLNetRecurrentTrainer(cfg)\n",
    "        # Prepare only eval data\n",
    "        _, eval_loader, eval_dataset = trainer.prepare_data()\n",
    "        print(f\"üìä Dataset prepared: {len(eval_loader)} batches\")\n",
    "        \n",
    "        # Run evaluation\n",
    "        metrics = trainer.evaluate(eval_loader, eval_dataset)\n",
    "        print(\"Metrics:\")\n",
    "        pprint(metrics)\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in scenario '{title}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Baseline: reproduce stage-2 evaluation conditions (short segments)\n",
    "baseline_metrics = run_eval_scenario(\n",
    "    title=\"Stage-2 settings (short-range, capped to 2 segments)\",\n",
    "    overrides={\n",
    "        # mirror stage-2 cap\n",
    "        \"max_n_segs\": 2,\n",
    "        # use saved flags (defaults), but make sure they're enabled as in Phase 2\n",
    "        \"use_global_softmax\": saved_cfg.get(\"use_global_softmax\", True),\n",
    "        \"use_any_positive_logic\": saved_cfg.get(\"use_any_positive_logic\", True),\n",
    "        \"no_answer_threshold\": saved_cfg.get(\"no_answer_threshold\", 1.5),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Lift segment cap at eval time to show coverage impact\n",
    "full_ctx_metrics = run_eval_scenario(\n",
    "    title=\"Lift segment cap at eval (max_n_segs=None)\",\n",
    "    overrides={\n",
    "        \"max_n_segs\": None,\n",
    "        \"use_global_softmax\": saved_cfg.get(\"use_global_softmax\", True),\n",
    "        \"use_any_positive_logic\": saved_cfg.get(\"use_any_positive_logic\", True),\n",
    "        \"no_answer_threshold\": saved_cfg.get(\"no_answer_threshold\", 1.5),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Stabilize logic: disable global softmax and any-positive\n",
    "stable_logic_metrics = run_eval_scenario(\n",
    "    title=\"Short segments but stabilized logic (global_softmax=False, any_positive=False)\",\n",
    "    overrides={\n",
    "        \"max_n_segs\": 2,\n",
    "        \"use_global_softmax\": False,\n",
    "        \"use_any_positive_logic\": False,\n",
    "        \"no_answer_threshold\": 1.0,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Threshold sweep to show sensitivity\n",
    "sens_results = {}\n",
    "for th in [1.5, 1.0, 0.5, 0.0]:\n",
    "    sens_results[th] = run_eval_scenario(\n",
    "        title=f\"Threshold sensitivity (no_answer_threshold={th})\",\n",
    "        overrides={\n",
    "            \"max_n_segs\": 2,\n",
    "            \"use_global_softmax\": saved_cfg.get(\"use_global_softmax\", True),\n",
    "            \"use_any_positive_logic\": saved_cfg.get(\"use_any_positive_logic\", True),\n",
    "            \"no_answer_threshold\": th,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä SUMMARY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if sens_results:\n",
    "    print(\"\\nüéØ Threshold Sensitivity (F1 scores):\")\n",
    "    for th, result in sens_results.items():\n",
    "        if result is not None:\n",
    "            f1 = result.get(\"f1\", \"N/A\")\n",
    "            print(f\"  Threshold {th}: F1 = {f1}\")\n",
    "\n",
    "print(\"\\nüìà All Scenarios Summary:\")\n",
    "scenarios = [\n",
    "    (\"Baseline (2 segments)\", baseline_metrics),\n",
    "    (\"Full context (no cap)\", full_ctx_metrics), \n",
    "    (\"Stabilized logic\", stable_logic_metrics)\n",
    "]\n",
    "\n",
    "for name, result in scenarios:\n",
    "    if result is not None:\n",
    "        f1 = result.get(\"f1\", \"N/A\")\n",
    "        em = result.get(\"exact_match\", \"N/A\")\n",
    "        loss = result.get(\"eval_loss\", \"N/A\")\n",
    "        print(f\"  {name}: F1={f1}, EM={em}, Loss={loss}\")\n",
    "    else:\n",
    "        print(f\"  {name}: Failed to run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}