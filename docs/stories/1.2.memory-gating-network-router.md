# Story 1.2: Memory Gating Network (Router)

## Status

Draft

## Story

**As a** research engineer,
**I want** to implement a learnable gating network that routes memory updates to appropriate experts,
**so that** the model can selectively update memory experts based on information content.

## Acceptance Criteria

1. **MemoryGatingNetwork class created** with learnable weight matrix `W_gate ∈ R^(k × d)`
2. **Routing computation** implemented: mean-pool memory write proposals → linear projection → softmax
3. **Temperature-controlled softmax** to prevent probability collapse (configurable temperature parameter)
4. **Numerical stability** handling for edge cases (all-zero inputs, extreme logits)
5. **Entropy regularization** hook for preventing routing collapse (optional, configurable)
6. **Unit tests** for routing probability computation, temperature scaling, numerical stability

## Integration Verification

**IV1**: Router operates independently without affecting existing memory update mechanisms
**IV2**: Routing network parameters correctly identified by optimizer groups
**IV3**: Model parameter count increases by expected amount (<100K for k=4, d=768)

## Tasks / Subtasks

- [ ] Create MemoryGatingNetwork class (AC: 1)
  - [ ] Create `src/gmmxlnet/models/gating_network.py`
  - [ ] Define `__init__(hidden_dim, num_experts, temperature, pooling_method)` constructor
  - [ ] Initialize learnable weight matrix W_gate ∈ R^(k × d) using `nn.Linear`
  - [ ] Add temperature parameter with validation (temperature > 0)
  - [ ] Add pooling method configuration (mean, max, attention)
- [ ] Implement routing computation (AC: 2)
  - [ ] Implement `forward(memory_write_hiddens)` method
  - [ ] Implement mean-pooling over memory write token positions
  - [ ] Implement linear projection: logits = W_gate @ pooled_hiddens
  - [ ] Implement temperature-scaled softmax: probs = softmax(logits / temperature)
  - [ ] Return (routing_probs, routing_logits, entropy)
- [ ] Add temperature-controlled softmax (AC: 3)
  - [ ] Implement configurable temperature scaling
  - [ ] Add temperature validation in constructor
  - [ ] Document temperature effects on routing sharpness
- [ ] Implement numerical stability handling (AC: 4)
  - [ ] Add epsilon to prevent division by zero
  - [ ] Handle all-zero input gracefully (uniform distribution fallback)
  - [ ] Clamp extreme logits to prevent NaN
  - [ ] Add unit test for edge cases
- [ ] Add entropy regularization hook (AC: 5)
  - [ ] Implement `compute_routing_entropy(routing_probs)` method
  - [ ] Implement `compute_load_balance_loss(routing_probs)` for Switch Transformer-style balancing
  - [ ] Make entropy regularization optional via configuration
  - [ ] Document entropy loss usage in training
- [ ] Implement accessor methods
  - [ ] Implement `get_routing_weights()` to access W_gate parameters
  - [ ] Implement `get_routing_distribution()` for analysis
- [ ] Create unit tests (AC: 6)
  - [ ] Create `tests/unit/test_gmm_routing.py`
  - [ ] Test routing probability computation and normalization (sum to 1.0)
  - [ ] Test temperature scaling effects
  - [ ] Test numerical stability with edge cases
  - [ ] Test entropy calculation correctness
  - [ ] Test load balance loss computation
  - [ ] Verify >= 90% coverage for gating_network.py
- [ ] Run integration verification (IV1-IV3)
  - [ ] Verify routing operates independently
  - [ ] Check optimizer can identify routing parameters
  - [ ] Count parameters and verify <100K for k=4, d=768

## Dev Notes

### Component Architecture

**MemoryGatingNetwork Responsibility:**
- Compute probability distribution over memory experts using content-based routing
- Receives memory write proposals from XLNet hidden states
- Outputs routing probabilities used for expert update modulation

**Key Interfaces:**
- `__init__(hidden_dim, num_experts, temperature, pooling_method)` - Initialize router
- `forward(memory_write_hiddens)` → (routing_probs, routing_logits, entropy) - Compute routing
- `get_routing_weights()` → Tensor - Access current routing distribution
- `compute_load_balance_loss(routing_probs)` → Tensor - Auxiliary loss for training

**Technology Stack:**
- PyTorch `nn.Linear` for learnable W_gate matrix (k × d)
- Pooling: Mean-pooling over memory write token positions
- Softmax with configurable temperature for numerical stability
- Optional entropy regularization hook

**Routing Computation Flow:**
```
memory_write_hiddens (batch, memory_slots, hidden_dim)
    ↓ mean-pool
pooled (batch, hidden_dim)
    ↓ W_gate projection
logits (batch, num_experts)
    ↓ temperature-scaled softmax
routing_probs (batch, num_experts)  [sum to 1.0 per batch item]
```

### Source Tree

**File Location:**
```
src/gmmxlnet/models/
├── memory_mixture.py              # Story 1.1 (already created)
└── gating_network.py              # ✨ NEW - This story
```

**Dependencies:**
- **Existing Components:** None (receives extracted hidden states)
- **New Components:** Outputs consumed by ExpertUpdater (Story 1.3) and AggregatedMemoryReader (Story 1.4)

### Coding Standards

**GMM-Specific Rules:**
- **Routing Stability:** Apply temperature scaling to all routing softmax operations; never use raw logits
- **Gradient Flow:** Ensure routing probabilities are differentiable (no detach() unless explicitly intended for analysis)
- **Numerical Safety:** Use torch.clamp() for routing weights to prevent NaN; add epsilon to denominators in entropy calculations
- **Error Messages:** Include routing mode and configuration in all error messages

**Naming Conventions:**
- Classes: PascalCase (e.g., `MemoryGatingNetwork`)
- Functions: snake_case (e.g., `compute_routing_probabilities`, `compute_load_balance_loss`)
- Routing variables: `routing_*` (e.g., `routing_logits`, `routing_probs`, `routing_entropy`)

**Configuration Validation:**
- Validate temperature > 0 at initialization
- Validate num_experts matches expected value

### Key Implementation Notes

**Temperature Scaling:**
- Temperature controls routing sharpness
- High temperature (>1.0) → uniform distribution (soft routing)
- Low temperature (<1.0) → peaked distribution (hard routing)
- Default: 1.0 (standard softmax)

**Numerical Stability:**
- All-zero inputs → return uniform distribution (1/k for each expert)
- Extreme logits → clamp to [-10, 10] before softmax
- Add epsilon=1e-10 to denominator in entropy calculations

**Load Balance Loss:**
- Encourages balanced expert usage across batch
- Formula: load_balance_loss = k × Σ(f_i × P_i) where f_i is fraction of samples routed to expert i
- Weight: typically 0.01 (from Switch Transformer paper)

### Testing

**Test File Location:**
- Create `tests/unit/test_gmm_routing.py`

**Testing Standards:**
- Framework: pytest 7.4.0+
- Coverage Target: >= 90% (routing is critical for expert specialization)

**Test Requirements:**
- Test routing computation with various input shapes
- Test probability normalization (sum to 1.0 per batch item, non-negative)
- Test temperature parameter effects (verify sharper/softer distributions)
- Test numerical stability:
  - All-zero inputs
  - Extreme logit values
  - NaN/Inf handling
- Test entropy calculation correctness and edge cases
- Test load balance loss computation

**Edge Cases to Test:**
- Batch size = 1
- num_experts = 2 (minimum)
- num_experts = 8 (maximum)
- temperature → 0 (numerical limits)
- All routing probabilities equal (maximum entropy)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story created from PRD | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
