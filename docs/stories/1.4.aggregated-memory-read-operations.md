# Story 1.4: Aggregated Memory Read Operations

## Status

Draft

## Story

**As a** research engineer,
**I want** to implement weighted aggregation of expert memories for read operations,
**so that** memory read tokens have access to collective knowledge from all specialized experts.

## Acceptance Criteria

1. **Weighted aggregation** implemented: `M_context = Σ(p_j · M_j)` for j=1 to k
2. **Routing mode support**: "write-based" (reuse write routing) and "read-based" (compute new routing)
3. **Read-specific routing** option with separate gating network for read operations
4. **Efficient computation** minimizing redundant routing calculations
5. **Memory replacement** logic to swap `[MEM_READ]` token embeddings with aggregated memory
6. **Unit tests** for aggregation computation, both routing modes, embedding replacement

## Integration Verification

**IV1**: Existing memory read operations (token-based, differentiable) work without modification
**IV2**: Read operation latency increases by less than 30% for k=4 experts
**IV3**: Aggregated memory shapes match expected dimensions for downstream processing

## Tasks / Subtasks

- [ ] Create AggregatedMemoryReader class (AC: 1, 2)
  - [ ] Create `src/gmmxlnet/models/memory_read.py`
  - [ ] Define `__init__(hidden_dim, num_experts, routing_mode)` constructor
  - [ ] Add routing_mode parameter: "write-based" or "read-based"
  - [ ] Initialize optional read-specific gating network (for read-based mode)
- [ ] Implement weighted aggregation (AC: 1)
  - [ ] Implement `forward(experts, routing_probs, read_hiddens_opt)` method
  - [ ] Compute M_context = Σ(p_j · M_j) for all experts
  - [ ] Validate aggregated memory shape matches expected (batch, memory_slots, hidden_dim)
  - [ ] Use efficient batched operations (avoid loops where possible)
- [ ] Implement routing mode support (AC: 2, 3)
  - [ ] Implement write-based mode: reuse cached routing_probs from write operation
  - [ ] Implement read-based mode: compute new routing from read_hiddens
  - [ ] Create `compute_read_routing(read_hiddens)` method for read-based mode
  - [ ] Add validation to ensure routing_probs or read_hiddens provided based on mode
- [ ] Implement memory embedding replacement (AC: 5)
  - [ ] Implement `replace_read_embeddings(sequence_output, aggregated_memory, read_positions)` method
  - [ ] Identify [MEM_READ] token positions in sequence
  - [ ] Replace embeddings at read positions with aggregated memory
  - [ ] Validate shapes before and after replacement
- [ ] Add efficient computation optimizations (AC: 4)
  - [ ] Use torch.einsum for weighted aggregation where applicable
  - [ ] Cache routing probs in write-based mode (avoid recomputation)
  - [ ] Implement batched operations for all k experts
  - [ ] Profile computation time vs single-expert baseline
- [ ] Create unit tests (AC: 6)
  - [ ] Create `tests/unit/test_gmm_memory_read.py`
  - [ ] Test weighted aggregation computation correctness
  - [ ] Test write-based routing mode
  - [ ] Test read-based routing mode
  - [ ] Test memory embedding replacement logic
  - [ ] Test batched aggregation efficiency
  - [ ] Test shape validation
  - [ ] Verify >= 85% coverage for memory_read.py
- [ ] Run integration verification (IV1-IV3)
  - [ ] Verify existing memory read operations still work
  - [ ] Profile read latency for k=4 experts (< 30% increase)
  - [ ] Verify aggregated memory shape compatibility

## Dev Notes

### Component Architecture

**AggregatedMemoryReader Responsibility:**
- Compute weighted aggregation of expert memories for read operations
- Replaces memory read token embeddings with aggregated context
- Supports both write-based (cached routing) and read-based (recomputed routing) modes

**Key Interfaces:**
- `__init__(hidden_dim, num_experts, routing_mode)` - Initialize reader
- `forward(experts, routing_probs, read_hiddens_opt)` → Tensor - Aggregated memory context
- `compute_read_routing(read_hiddens)` → Tensor - Optional read-specific routing
- `replace_read_embeddings(sequence_output, aggregated_memory)` → Tensor - Embedding replacement

**Technology Stack:**
- Weighted sum: M_context = Σ(p_j · M_j) for j=1 to k
- Optional separate gating network for read-specific routing
- Efficient batched aggregation using PyTorch broadcasting

**Aggregation Flow:**
```
expert_states: List[(batch, memory_slots, hidden_dim)] × k experts
routing_probs: (batch, k)

# Reshape for broadcasting
routing_probs: (batch, k, 1, 1)
expert_stack: (batch, k, memory_slots, hidden_dim)

# Weighted sum
M_context = Σ(routing_probs[:, j] * expert_stack[:, j])
         → (batch, memory_slots, hidden_dim)
```

### Source Tree

**File Location:**
```
src/gmmxlnet/models/
├── memory_mixture.py              # Story 1.1
├── gating_network.py              # Story 1.2
├── expert_updates.py              # Story 1.3
└── memory_read.py                 # ✨ NEW - This story
```

**Dependencies:**
- **Existing Components:** Integrates with XLNet sequence output tensor manipulation
- **New Components:** Receives expert states from `GatedMemoryMixture` (Story 1.1), routing from `MemoryGatingNetwork` (Story 1.2)

### Coding Standards

**GMM-Specific Rules:**
- **Memory Shapes:** Validate aggregated memory matches expected dimensions
- **Efficient Computation:** Use batched operations; avoid Python loops over experts
- **State Management:** Never mutate input expert states
- **Error Messages:** Include routing mode and expert count in error messages

**Naming Conventions:**
- Functions: snake_case (e.g., `compute_read_routing`, `replace_read_embeddings`)
- Variables: descriptive (e.g., `aggregated_memory`, `read_positions`)

### Key Implementation Notes

**Routing Modes:**

1. **Write-based (default):**
   - Reuse routing probabilities from write operation
   - More efficient (no additional routing computation)
   - Assumes read should use same expert mix as write
   - Good for: consistent memory operations

2. **Read-based (optional):**
   - Compute new routing based on query/read context
   - Additional gating network required
   - Allows different expert emphasis for reads vs writes
   - Good for: adaptive retrieval

**Efficient Aggregation:**
```python
# Stack experts for efficient batch operation
expert_stack = torch.stack(expert_states, dim=1)  # (batch, k, memory_slots, hidden_dim)

# Reshape routing for broadcasting
routing_expanded = routing_probs.unsqueeze(-1).unsqueeze(-1)  # (batch, k, 1, 1)

# Weighted sum (single operation)
aggregated = (expert_stack * routing_expanded).sum(dim=1)  # (batch, memory_slots, hidden_dim)
```

**Memory Replacement:**
- Identify [MEM_READ] token positions in sequence
- Replace embeddings at those positions with aggregated_memory
- Maintain sequence length and shape
- Similar to existing memory token handling in MemXLNet

### Testing

**Test File Location:**
- Create `tests/unit/test_gmm_memory_read.py`

**Testing Standards:**
- Framework: pytest 7.4.0+
- Coverage Target: >= 85%

**Test Requirements:**
- Test weighted aggregation correctness:
  - Manual calculation vs implementation
  - Various expert counts (k=2, 4, 8)
  - Various batch sizes
- Test routing modes:
  - Write-based: verify cached routing used
  - Read-based: verify new routing computed
  - Mode switching
- Test embedding replacement:
  - Correct positions identified
  - Shapes preserved
  - No data corruption in non-read positions
- Test efficiency:
  - Profile aggregation time
  - Compare to naive loop implementation

**Edge Cases:**
- Single expert selected (p_j=1 for one expert)
- Uniform routing (all experts equally weighted)
- No [MEM_READ] tokens (edge case handling)
- Batch size = 1

### Performance Target

**IV2 Requirement:** Read operation latency increase < 30% for k=4 experts

**Baseline:** Single-expert memory read time
**Target:** GMM aggregated read with k=4 experts should be < 1.3× baseline

**Optimization strategies:**
- Use torch.einsum for weighted sum
- Batch all k experts in single tensor operation
- Cache routing in write-based mode
- Avoid Python loops

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story created from PRD | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
