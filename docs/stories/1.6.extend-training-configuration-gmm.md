# Story 1.6: Extend Training Configuration for GMM Support

## Status

Draft

## Story

**As a** research engineer,
**I want** to add GMM-specific configuration parameters to TrainingConfig,
**so that** users can train GMM models through standard training scripts with configuration changes only.

## Acceptance Criteria

1. **GMMTrainingConfig class created** extending TrainingConfig with GMM parameters
2. **Configuration parameters added**: `use_gmm_memory`, `num_memory_experts`, `routing_temperature`, `entropy_regularization_weight`, `load_balance_weight`
3. **Default configurations** provided: gmm-small (k=2), gmm-balanced (k=4), gmm-large (k=8)
4. **Validation logic** ensuring parameter compatibility (e.g., k must be in [2,8], temperature > 0)
5. **Warmup strategy compatibility** verified with existing `warmup_freeze_base_epochs`
6. **Integration test** training GMM model for 1 epoch with minimal data

## Integration Verification

**IV1**: Existing training configurations work without modification (backward compatibility)
**IV2**: Training with GMM enabled completes without errors on toy dataset
**IV3**: Configuration JSON saved alongside checkpoint contains GMM parameters

## Tasks / Subtasks

- [ ] Create GMMTrainingConfig class (AC: 1, 2)
  - [ ] Create `src/gmmxlnet/training/__init__.py`
  - [ ] Create `src/gmmxlnet/training/config.py`
  - [ ] Define GMMTrainingConfig extending TrainingConfig
  - [ ] Add GMM-specific parameters as dataclass fields
  - [ ] Import TrainingConfig from memxlnet.training
- [ ] Add GMM configuration parameters (AC: 2)
  - [ ] Add `use_gmm_memory: bool = False`
  - [ ] Add `num_memory_experts: int = 4`
  - [ ] Add `routing_temperature: float = 1.0`
  - [ ] Add `routing_mode: str = "write-based"`
  - [ ] Add `entropy_regularization_weight: float = 0.0`
  - [ ] Add `load_balance_weight: float = 0.01`
  - [ ] Add expert_init_strategies: List[str] = ["learned"] * k
- [ ] Create default configurations (AC: 3)
  - [ ] Create `gmm_small_config()` factory: k=2, default params
  - [ ] Create `gmm_balanced_config()` factory: k=4, balanced params
  - [ ] Create `gmm_large_config()` factory: k=8, large capacity
  - [ ] Document parameter choices for each preset
- [ ] Implement validation logic (AC: 4)
  - [ ] Add `__post_init__()` method
  - [ ] Validate num_memory_experts in [2, 8]
  - [ ] Validate routing_temperature > 0
  - [ ] Validate routing_mode in ["write-based", "read-based"]
  - [ ] Validate load_balance_weight >= 0
  - [ ] Validate entropy_regularization_weight >= 0
  - [ ] Raise ValueError with clear messages for invalid configs
- [ ] Extend configuration serialization (AC: 1)
  - [ ] Add `to_dict()` method including GMM parameters
  - [ ] Add `from_dict()` class method for deserialization
  - [ ] Ensure JSON serialization compatible
  - [ ] Add "memory_type": "gmm" metadata field
- [ ] Verify warmup compatibility (AC: 5)
  - [ ] Test config with warmup_freeze_base_epochs > 0
  - [ ] Test config with warmup_disable_global_softmax_epochs > 0
  - [ ] Verify GMM parameters don't conflict with warmup strategies
  - [ ] Document GMM-specific warmup considerations
- [ ] Create training integration test (AC: 6)
  - [ ] Create `tests/integration/test_gmm_training_config.py`
  - [ ] Create toy dataset (10 examples)
  - [ ] Initialize GMMXLNetForQA with GMMTrainingConfig
  - [ ] Run 1 epoch of training
  - [ ] Verify training completes without errors
  - [ ] Verify loss decreases
  - [ ] Verify routing statistics valid
- [ ] Run integration verification (IV1-IV3)
  - [ ] Test existing TrainingConfig still works
  - [ ] Test GMM training on toy dataset completes
  - [ ] Verify config.json contains GMM parameters after save

## Dev Notes

### Component Architecture

**GMMTrainingConfig Responsibility:**
- Extend existing TrainingConfig with GMM-specific parameters
- Provide validation for GMM parameter combinations
- Support serialization/deserialization for checkpoint saving
- Offer preset configurations for common use cases

**Configuration Parameters:**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `use_gmm_memory` | bool | False | Enable GMM memory system |
| `num_memory_experts` | int | 4 | Number of memory experts (k) |
| `routing_temperature` | float | 1.0 | Temperature for routing softmax |
| `routing_mode` | str | "write-based" | Routing mode for read operations |
| `entropy_regularization_weight` | float | 0.0 | Weight for entropy regularization loss |
| `load_balance_weight` | float | 0.01 | Weight for load balance loss |
| `expert_init_strategies` | List[str] | ["learned"]*k | Initialization strategy per expert |

**Preset Configurations:**

1. **gmm-small (k=2):**
   - Minimal expert count for testing
   - Lower computational overhead
   - Good for: prototyping, limited GPU memory

2. **gmm-balanced (k=4):**
   - Balanced expert count
   - Moderate computational cost
   - Good for: most research experiments

3. **gmm-large (k=8):**
   - Maximum expert count
   - Highest capacity and specialization
   - Good for: production models, large-scale experiments

### Source Tree

**File Location:**
```
src/gmmxlnet/training/
├── __init__.py                    # Export GMMTrainingConfig, presets
└── config.py                      # ✨ NEW - This story
```

**Import Pattern:**
```python
from memxlnet.training import TrainingConfig
from dataclasses import dataclass, field
from typing import List, Optional
```

### Coding Standards

**Dataclass Pattern:**
```python
@dataclass
class GMMTrainingConfig(TrainingConfig):
    """Training configuration for GMM-XLNet models."""

    # GMM-specific parameters
    use_gmm_memory: bool = False
    num_memory_experts: int = 4
    routing_temperature: float = 1.0
    routing_mode: str = "write-based"
    entropy_regularization_weight: float = 0.0
    load_balance_weight: float = 0.01
    expert_init_strategies: List[str] = field(default_factory=lambda: None)

    def __post_init__(self):
        super().__post_init__()  # Call parent validation

        # GMM-specific validation
        if self.use_gmm_memory:
            self._validate_gmm_params()

    def _validate_gmm_params(self):
        # Validation logic here
        pass
```

**Validation Requirements:**
- num_memory_experts in [2, 4, 8] (power of 2 for efficiency)
- routing_temperature > 0
- routing_mode in ["write-based", "read-based"]
- All weight parameters >= 0
- expert_init_strategies length matches num_memory_experts

**Error Messages:**
- Include parameter name, provided value, and valid range
- Example: "num_memory_experts must be in [2, 8], got 16"

### Key Implementation Notes

**Serialization Format:**
```json
{
    "memory_type": "gmm",
    "num_memory_experts": 4,
    "routing_temperature": 1.0,
    "routing_mode": "write-based",
    "entropy_regularization_weight": 0.0,
    "load_balance_weight": 0.01,
    "expert_init_strategies": ["learned", "learned", "learned", "learned"],
    ... (other TrainingConfig fields)
}
```

**Preset Factory Functions:**
```python
def gmm_small_config(**kwargs):
    """Factory for small GMM configuration (k=2)."""
    return GMMTrainingConfig(
        use_gmm_memory=True,
        num_memory_experts=2,
        routing_temperature=1.0,
        **kwargs
    )
```

### Testing

**Test File Location:**
- Create `tests/integration/test_gmm_training_config.py`

**Test Requirements:**
- Test each preset configuration creates valid config
- Test validation catches invalid parameters
- Test serialization/deserialization round-trip
- Test backward compatibility with TrainingConfig
- Test training integration with minimal data

**Validation Test Cases:**
- Valid configs: k=2, 4, 8
- Invalid configs: k=0, 1, 16, -1
- Invalid temperature: 0, -1
- Invalid routing mode: "invalid-mode"
- Invalid weights: -0.1

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story created from PRD | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
