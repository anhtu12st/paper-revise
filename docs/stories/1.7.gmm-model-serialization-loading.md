# Story 1.7: GMM Model Serialization and Loading

## Status

Draft

## Story

**As a** research engineer,
**I want** to implement proper save/load mechanisms for GMM models,
**so that** trained GMM models can be checkpointed, resumed, and shared via HuggingFace Hub.

## Acceptance Criteria

1. **Save method extended** to serialize all expert states, gating network parameters, and GMM config
2. **Load method extended** to deserialize experts and reconstruct routing networks
3. **Version detection** to identify GMM vs non-GMM checkpoints during loading
4. **Backward compatibility** ensuring old checkpoints load without GMM parameters
5. **HuggingFace Hub compatibility** verified with upload/download round-trip
6. **Unit tests** for save/load with various expert counts and configurations

## Integration Verification

**IV1**: Existing non-GMM checkpoints load correctly and evaluate with expected metrics
**IV2**: GMM checkpoints uploaded to Hub download successfully on different machines
**IV3**: Loaded GMM models produce deterministic outputs matching saved checkpoint

## Tasks / Subtasks

- [ ] Extend save_pretrained method (AC: 1)
  - [ ] Add save logic to GMMXLNetForQA
  - [ ] Serialize all expert state parameters
  - [ ] Serialize gating network weights
  - [ ] Serialize expert updater parameters
  - [ ] Serialize memory reader parameters
  - [ ] Save GMM configuration to config.json
  - [ ] Add "memory_type": "gmm" metadata
- [ ] Implement checkpoint structure (AC: 1)
  - [ ] Save expert states: expert_0.pt, expert_1.pt, ..., expert_k-1.pt
  - [ ] Save routing network: routing_network.pt
  - [ ] Save config.json with GMM parameters
  - [ ] Maintain compatibility with HuggingFace Hub structure
- [ ] Extend from_pretrained method (AC: 2, 3)
  - [ ] Add load logic to GMMXLNetForQA
  - [ ] Read config.json and detect memory_type
  - [ ] Load expert states from checkpoint files
  - [ ] Reconstruct gating network from saved weights
  - [ ] Reconstruct expert updater with correct expert count
  - [ ] Reconstruct memory reader with routing mode
  - [ ] Validate loaded model matches saved configuration
- [ ] Implement version detection (AC: 3, 4)
  - [ ] Check config.json for "memory_type" field
  - [ ] If "gmm": load GMM-specific parameters
  - [ ] If missing or "standard": skip GMM loading (backward compatible)
  - [ ] Raise clear error if mismatch (e.g., GMM checkpoint into non-GMM model)
  - [ ] Add version field for future compatibility
- [ ] Add validation after loading (AC: 2)
  - [ ] Verify expert count matches configuration
  - [ ] Verify routing network shape matches expected
  - [ ] Verify all expert states have correct shapes
  - [ ] Run sanity forward pass on dummy input
- [ ] Implement HuggingFace Hub integration (AC: 5)
  - [ ] Test upload with hub.push_to_hub()
  - [ ] Test download with from_pretrained(hub_id)
  - [ ] Verify round-trip: save → upload → download → load
  - [ ] Test with different hub_id formats
  - [ ] Add Hub model card template
- [ ] Create unit tests (AC: 6)
  - [ ] Create `tests/unit/test_gmm_serialization.py`
  - [ ] Test save/load round-trip with k=2, 4, 8
  - [ ] Test loading non-GMM checkpoint (backward compatibility)
  - [ ] Test loading GMM checkpoint with wrong expert count (error)
  - [ ] Test Hub upload/download round-trip
  - [ ] Test deterministic outputs after loading
  - [ ] Verify >= 80% coverage for serialization code
- [ ] Run integration verification (IV1-IV3)
  - [ ] Load and evaluate existing non-GMM checkpoint
  - [ ] Upload GMM checkpoint to test Hub repository
  - [ ] Download and verify outputs match

## Dev Notes

### Component Architecture

**Serialization Responsibility:**
- Save all GMM component parameters to disk
- Maintain HuggingFace Hub compatibility
- Support version detection for backward compatibility
- Enable checkpoint resumption and sharing

**Checkpoint Structure:**
```
checkpoint_dir/
├── config.json                    # Model configuration with GMM params
├── pytorch_model.bin              # Main model weights
│   ├── base_model.*               # XLNet base parameters
│   ├── expert_mixture.*           # GatedMemoryMixture parameters
│   ├── routing_network.*          # MemoryGatingNetwork parameters
│   ├── expert_updater.*           # ExpertUpdater parameters
│   ├── memory_reader.*            # AggregatedMemoryReader parameters
│   └── qa_head.*                  # QA head parameters
└── README.md                      # Model card (optional)
```

**Config.json Format:**
```json
{
    "model_type": "gmmxlnet",
    "memory_type": "gmm",
    "num_memory_experts": 4,
    "routing_temperature": 1.0,
    "routing_mode": "write-based",
    "memory_slots": 16,
    "hidden_dim": 768,
    "version": "1.0",
    ...
}
```

### Source Tree

**Files Modified:**
```
src/gmmxlnet/models/
└── gmm_xlnet_qa.py                # Extend save_pretrained, from_pretrained
```

**Test Files:**
```
tests/unit/
└── test_gmm_serialization.py      # ✨ NEW - This story
```

### Coding Standards

**Save Method Signature:**
```python
def save_pretrained(
    self,
    save_directory: str,
    push_to_hub: bool = False,
    **kwargs
):
    """Save GMM-XLNet model to directory."""
    # Implementation
```

**Load Method Signature:**
```python
@classmethod
def from_pretrained(
    cls,
    pretrained_model_name_or_path: str,
    **kwargs
):
    """Load GMM-XLNet model from checkpoint or Hub."""
    # Implementation
```

### Key Implementation Notes

**State Dict Organization:**
```python
state_dict = {
    # Expert mixture
    "expert_mixture.expert_0": Tensor,
    "expert_mixture.expert_1": Tensor,
    ...
    # Routing network
    "routing_network.W_gate.weight": Tensor,
    "routing_network.W_gate.bias": Tensor,
    # Expert updater (per-expert gates)
    "expert_updater.gate_networks.0.weight": Tensor,
    "expert_updater.update_networks.0.weight": Tensor,
    ...
    # Memory reader
    "memory_reader.read_gating_network.weight": Tensor,  # if read-based
    ...
}
```

**Version Detection Logic:**
```python
def from_pretrained(cls, model_path):
    config = load_config(f"{model_path}/config.json")

    memory_type = config.get("memory_type", "standard")

    if memory_type == "gmm":
        # Load GMM-specific components
        model = cls.load_gmm_model(model_path, config)
    elif memory_type == "standard":
        # Load standard MemXLNet model
        model = cls.load_standard_model(model_path, config)
    else:
        raise ValueError(f"Unknown memory_type: {memory_type}")

    return model
```

**Backward Compatibility:**
- Old checkpoints don't have "memory_type" field → treat as "standard"
- GMM-specific parameters missing → skip GMM loading
- Raise clear error if user tries to load GMM checkpoint into non-GMM model class

**Error Handling:**
- Clear error message if expert count mismatch
- Clear error message if routing mode incompatible
- Suggest corrective action (e.g., "Use GMMXLNetForQA.from_pretrained() instead")

### Testing

**Test File Location:**
- Create `tests/unit/test_gmm_serialization.py`

**Testing Standards:**
- Framework: pytest 7.4.0+
- Coverage Target: >= 80%

**Test Requirements:**
- **Round-trip test:**
  - Create model, save, load, verify outputs identical
  - Test with k=2, 4, 8
  - Test with different routing modes
- **Backward compatibility:**
  - Load non-GMM checkpoint → should not fail
  - Verify standard model behavior preserved
- **Error handling:**
  - Load GMM checkpoint with wrong expert count → clear error
  - Load non-GMM checkpoint into GMM model class → fallback or error
- **HuggingFace Hub:**
  - Upload test checkpoint to private repo
  - Download and verify outputs match
  - Clean up test repo after test

**Determinism Test:**
```python
def test_deterministic_outputs():
    # Create model
    model = GMMXLNetForQA(...)
    input_data = create_toy_input()

    # Get outputs before save
    outputs_before = model(input_data)

    # Save and load
    model.save_pretrained("test_checkpoint")
    loaded_model = GMMXLNetForQA.from_pretrained("test_checkpoint")

    # Get outputs after load
    outputs_after = loaded_model(input_data)

    # Verify identical
    assert torch.allclose(outputs_before.logits, outputs_after.logits)
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story created from PRD | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
