# Story 1.3: Gated Expert Updates with Router Modulation

## Status

Draft

## Story

**As a** research engineer,
**I want** to implement memory update logic that modulates LSTM-style gates with router probabilities,
**so that** experts are selectively updated based on routing decisions while maintaining stable learning.

## Acceptance Criteria

1. **Expert-specific gated updates** implemented: `M_j^(i) = (p_j · g_j) ⊙ u_j + (1 - p_j · g_j) ⊙ M_j^(i-1)`
2. **Per-expert gate networks** created with separate parameters for each expert
3. **Routing modulation** correctly applied to gate activations before memory update
4. **Gradient flow** verified through routing probabilities to gate networks
5. **Memory protection mechanism** ensuring low-probability experts preserve their state
6. **Unit tests** for update computation, routing modulation, gradient flow

## Integration Verification

**IV1**: Standard gated updates (non-GMM) continue to work with existing tests passing
**IV2**: Memory state shapes remain consistent across update operations
**IV3**: No memory leaks or GPU memory growth during repeated updates

## Tasks / Subtasks

- [ ] Create ExpertUpdater class (AC: 1, 2)
  - [ ] Create `src/gmmxlnet/models/expert_updates.py`
  - [ ] Define `__init__(hidden_dim, num_experts)` constructor
  - [ ] Initialize `nn.ModuleList` of per-expert gate networks
  - [ ] Initialize `nn.ModuleList` of per-expert update networks
  - [ ] Create separate W_g (gate) and W_u (update) matrices per expert
- [ ] Implement gated update computation (AC: 1)
  - [ ] Implement `forward(expert_states, write_hiddens, routing_probs)` method
  - [ ] For each expert: concatenate M_j_prev with write_hiddens
  - [ ] Compute per-expert LSTM-style gates: g_j = sigmoid(W_g[combined])
  - [ ] Compute per-expert updates: u_j = tanh(W_u[combined])
  - [ ] Extract routing probability p_j for each expert
  - [ ] Apply formula: M_j_next = (p_j · g_j) ⊙ u_j + (1 - p_j · g_j) ⊙ M_j_prev
  - [ ] Return list of updated expert states
- [ ] Implement routing modulation (AC: 3)
  - [ ] Implement `apply_routing_modulation(gates, routing_probs)` method
  - [ ] Reshape routing probs for broadcasting: (batch,) → (batch, 1, 1)
  - [ ] Element-wise multiply: modulated_gate = p_j * g_j
  - [ ] Validate modulated gate values in [0, 1] range
- [ ] Add helper methods
  - [ ] Implement `compute_expert_gates(expert_state, write_hidden)` for single expert
  - [ ] Implement `_validate_shapes(expert_states, write_hiddens, routing_probs)`
  - [ ] Add assertions for shape consistency
- [ ] Verify gradient flow (AC: 4)
  - [ ] Create test that backward pass computes gradients for routing network
  - [ ] Verify gradients flow through routing probabilities
  - [ ] Verify gradients flow to gate networks
  - [ ] Test that requires_grad=True for routing_probs
- [ ] Verify memory protection (AC: 5)
  - [ ] Test with p_j ≈ 0: verify M_j unchanged
  - [ ] Test with p_j ≈ 1: verify standard LSTM update behavior
  - [ ] Test with p_j = 0.5: verify partial update
- [ ] Create unit tests (AC: 6)
  - [ ] Create `tests/unit/test_gmm_expert_updates.py`
  - [ ] Test per-expert gate computation (g_j, u_j)
  - [ ] Test routing modulation application
  - [ ] Test memory protection mechanism
  - [ ] Test gradient flow through routing probabilities
  - [ ] Test update consistency across different expert counts
  - [ ] Test shape validation
  - [ ] Verify >= 85% coverage for expert_updates.py
- [ ] Run integration verification (IV1-IV3)
  - [ ] Run existing MemXLNet tests to verify no regression
  - [ ] Verify memory state shapes consistent
  - [ ] Profile GPU memory usage during repeated updates

## Dev Notes

### Component Architecture

**ExpertUpdater Responsibility:**
- Implement expert-specific gated LSTM-style updates modulated by routing probabilities
- Applies routing-modulated updates: (p_j · g_j) ⊙ u_j + (1 - p_j · g_j) ⊙ M_j^(i-1)
- Integrates with existing memory update logic

**Key Interfaces:**
- `__init__(hidden_dim, num_experts)` - Initialize per-expert gate networks
- `forward(experts, write_hiddens, routing_probs)` → List[Tensor] - Updated expert states
- `compute_expert_gates(expert_state, write_hidden)` → (gate, update) - Per-expert LSTM computation
- `apply_routing_modulation(gates, routing_probs)` → Tensor - Modulate gates with routing

**Technology Stack:**
- PyTorch `nn.ModuleList` of per-expert gate networks
- Each expert has separate W_g (gate) and W_u (update) parameter matrices
- Gradient flow through routing probabilities to gating network

### Step-by-Step Update Logic

```python
def forward(self, expert_states, write_hiddens, routing_probs):
    """
    Apply routing-modulated gated updates to all experts.

    Args:
        expert_states: List[Tensor] - k expert memories, each (batch, memory_slots, hidden_dim)
        write_hiddens: Tensor - (batch, memory_slots, hidden_dim) - Proposed memory update
        routing_probs: Tensor - (batch, k) - Routing probabilities from gating network

    Returns:
        List[Tensor] - k updated expert memories
    """
    updated_experts = []

    for j in range(self.num_experts):
        # Step 1: Concatenate previous expert state with new proposal
        M_j_prev = expert_states[j]  # (batch, memory_slots, hidden_dim)
        combined = torch.cat([M_j_prev, write_hiddens], dim=-1)  # (batch, memory_slots, 2*hidden_dim)

        # Step 2: Compute per-expert LSTM-style gates
        g_j = torch.sigmoid(self.gate_networks[j](combined))  # (batch, memory_slots, hidden_dim)
        u_j = torch.tanh(self.update_networks[j](combined))   # (batch, memory_slots, hidden_dim)

        # Step 3: Extract routing probability for this expert
        p_j = routing_probs[:, j]  # (batch,)
        p_j = p_j.view(-1, 1, 1)   # Broadcast: (batch, 1, 1)

        # Step 4: Modulate gate with routing probability
        modulated_gate = p_j * g_j  # (batch, memory_slots, hidden_dim)

        # Step 5: Apply gated update with routing modulation
        M_j_next = modulated_gate * u_j + (1 - modulated_gate) * M_j_prev

        updated_experts.append(M_j_next)

    return updated_experts
```

**Key Insight:** When `p_j ≈ 0` (expert not selected), `modulated_gate ≈ 0`, so `M_j_next ≈ M_j_prev` (memory preserved). When `p_j ≈ 1` (expert strongly selected), update behaves like standard LSTM gate.

### Source Tree

**File Location:**
```
src/gmmxlnet/models/
├── memory_mixture.py              # Story 1.1
├── gating_network.py              # Story 1.2
└── expert_updates.py              # ✨ NEW - This story
```

**Dependencies:**
- **Existing Components:** Reuses LSTM gating logic patterns from existing `MemXLNetForQA`
- **New Components:** Receives routing from `MemoryGatingNetwork` (Story 1.2), states from `GatedMemoryMixture` (Story 1.1)

### Coding Standards

**GMM-Specific Rules:**
- **Memory Shapes:** All expert memories maintain identical shape; validate in forward pass
- **Gradient Flow:** Ensure routing probabilities are differentiable (no detach())
- **State Management:** Always return new memory states (never mutate in-place)
- **Error Messages:** Include expert index and configuration in error messages

**Naming Conventions:**
- Functions: snake_case (e.g., `compute_expert_gates`, `apply_routing_modulation`)
- Expert variables: `expert_*` or `*_j` (e.g., `expert_states`, `gate_j`, `update_j`)
- Private methods: `_leading_underscore` (e.g., `_validate_shapes`)

### Key Implementation Notes

**Per-Expert Networks:**
- Each expert has independent gate and update networks
- Separate parameters allow expert-specific gating behavior
- ModuleList ensures parameters tracked by optimizer

**Routing Modulation:**
- Routing probability p_j acts as "soft switch" for expert j
- Modulation formula: (p_j · g_j) creates routing-aware gate
- When p_j=0: gate fully closed (memory preserved)
- When p_j=1: gate controlled by learned g_j (standard LSTM)

**Shape Broadcasting:**
- routing_probs: (batch, k) → need (batch, 1, 1) for broadcasting
- Broadcast across memory_slots and hidden_dim dimensions
- Ensures p_j multiplies all positions in expert memory

### Testing

**Test File Location:**
- Create `tests/unit/test_gmm_expert_updates.py`

**Testing Standards:**
- Framework: pytest 7.4.0+
- Coverage Target: >= 85%

**Test Requirements:**
- Test per-expert gate computation with various inputs
- Test routing modulation correctly applies probabilities
- Test memory protection (p_j=0 preserves state, p_j=1 updates normally)
- Test gradient flow:
  - Create dummy loss from updated states
  - Call backward()
  - Verify routing_probs.grad is not None
  - Verify gate network parameters have gradients
- Test update consistency across expert counts (k=2, 4, 8)
- Test shape validation catches mismatches

**Edge Cases:**
- All routing probs = 0 (all experts frozen)
- All routing probs = 1 (all experts update)
- Single expert gets p_j=1, rest get 0 (single expert routing)
- Batch size = 1

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story created from PRD | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
