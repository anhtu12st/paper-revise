# Story 1.9: Comprehensive GMM Test Suite

## Status

Draft

## Story

**As a** research engineer,
**I want** to implement thorough unit and integration tests for GMM functionality,
**so that** GMM implementation is robust, reliable, and maintains quality standards.

## Acceptance Criteria

1. **Unit tests for GMMMemory**: expert initialization, state management, shape validation
2. **Unit tests for MemoryGatingNetwork**: routing computation, numerical stability, temperature scaling
3. **Unit tests for gated updates**: routing modulation, gradient flow, memory protection
4. **Integration test**: full training loop with GMM on toy dataset (10 examples, 2 epochs)
5. **Integration test**: evaluation pipeline with GMM memory state propagation
6. **Regression test**: verify all existing tests still pass with GMM code added
7. **Test coverage**: ≥80% coverage for all GMM-related code

## Integration Verification

**IV1**: Entire test suite passes with GMM code present but disabled (`use_gmm_memory=False`)
**IV2**: No test execution time increase >10% for non-GMM tests
**IV3**: CI/CD pipeline (if present) successfully runs all tests

## Tasks / Subtasks

- [ ] Consolidate and verify unit tests (AC: 1, 2, 3)
  - [ ] Verify `tests/unit/test_gmm_memory.py` exists (Story 1.1)
  - [ ] Verify `tests/unit/test_gmm_routing.py` exists (Story 1.2)
  - [ ] Verify `tests/unit/test_gmm_expert_updates.py` exists (Story 1.3)
  - [ ] Verify `tests/unit/test_gmm_memory_read.py` exists (Story 1.4)
  - [ ] Verify `tests/unit/test_gmm_serialization.py` exists (Story 1.7)
  - [ ] Verify `tests/unit/test_gmm_analysis.py` exists (Story 1.8)
  - [ ] Run all unit tests and verify pass
- [ ] Create training integration test (AC: 4)
  - [ ] Create or verify `tests/integration/test_gmm_training.py`
  - [ ] Create toy dataset fixture (10 examples, 2 segments each)
  - [ ] Initialize GMMXLNetForQA with k=4 experts
  - [ ] Configure GMMTrainingConfig with minimal epochs
  - [ ] Run 2 epochs of training
  - [ ] Assert loss decreases
  - [ ] Assert routing probs valid (sum to 1, non-NaN)
  - [ ] Assert memory states propagate correctly
  - [ ] Assert checkpoint save/load works
  - [ ] Mark as `@pytest.mark.integration` and `@pytest.mark.slow`
- [ ] Create evaluation integration test (AC: 5)
  - [ ] Create or extend `tests/integration/test_gmm_evaluation.py`
  - [ ] Load pre-trained GMM model (or train minimal one)
  - [ ] Run evaluation on toy validation set
  - [ ] Assert evaluation metrics computed (EM, F1)
  - [ ] Assert time-step-major batching works with GMM
  - [ ] Assert routing statistics collected correctly
  - [ ] Mark as `@pytest.mark.integration`
- [ ] Create regression test suite (AC: 6)
  - [ ] Create `tests/regression/test_gmm_regression.py`
  - [ ] Test existing MemXLNet tests with GMM code present
  - [ ] Test token-based memory still works
  - [ ] Test differentiable memory still works
  - [ ] Test Phase2Trainer still works
  - [ ] Test evaluation still works
  - [ ] Assert no functionality changes
  - [ ] Mark as `@pytest.mark.regression`
- [ ] Add edge case tests
  - [ ] Test GMM with k=2 (minimum experts)
  - [ ] Test GMM with k=8 (maximum experts)
  - [ ] Test batch_size=1
  - [ ] Test single segment (no memory propagation)
  - [ ] Test empty routing (all probs=0, edge case)
  - [ ] Test uniform routing (all probs=1/k)
- [ ] Add error handling tests
  - [ ] Test invalid expert count (k=0, k=1, k=16)
  - [ ] Test invalid temperature (temp=0, temp=-1)
  - [ ] Test shape mismatch errors
  - [ ] Test checkpoint loading errors (wrong expert count)
  - [ ] Assert clear error messages for all failures
- [ ] Measure test coverage (AC: 7)
  - [ ] Run pytest-cov on gmmxlnet module
  - [ ] Generate coverage report
  - [ ] Identify uncovered lines
  - [ ] Add tests for uncovered critical paths
  - [ ] Verify >= 80% coverage achieved
  - [ ] Generate HTML coverage report
- [ ] Optimize test performance
  - [ ] Profile slow tests
  - [ ] Use smaller models for unit tests (e.g., hidden_dim=64)
  - [ ] Cache fixtures where possible
  - [ ] Parallelize independent tests
- [ ] Run integration verification (IV1-IV3)
  - [ ] Run all tests with use_gmm_memory=False
  - [ ] Profile test execution time
  - [ ] Verify CI compatibility

## Dev Notes

### Testing Organization

**Test Structure:**
```
tests/
├── conftest.py                        # Shared fixtures
├── unit/                              # Unit tests (Stories 1.1-1.4, 1.7-1.8)
│   ├── test_gmm_memory.py
│   ├── test_gmm_routing.py
│   ├── test_gmm_expert_updates.py
│   ├── test_gmm_memory_read.py
│   ├── test_gmm_serialization.py
│   └── test_gmm_analysis.py
├── integration/                       # Integration tests (this story)
│   ├── test_gmm_training.py          # ✨ NEW or verify
│   ├── test_gmm_evaluation.py        # ✨ NEW or verify
│   └── test_gmm_integration.py       # Story 1.5
└── regression/                        # Regression tests (this story)
    └── test_gmm_regression.py         # ✨ NEW
```

### Shared Test Fixtures

**Create in conftest.py:**
```python
import pytest
import torch
from gmmxlnet.models import GMMXLNetForQA
from gmmxlnet.training import GMMTrainingConfig

@pytest.fixture
def toy_gmm_model():
    """Small GMM model for testing."""
    config = {
        "num_experts": 4,
        "memory_slots": 8,
        "hidden_dim": 64,  # Small for speed
        "routing_temperature": 1.0,
    }
    return GMMXLNetForQA(**config)

@pytest.fixture
def toy_dataset():
    """Minimal dataset for integration tests."""
    # Create 10 examples with 2 segments each
    return create_toy_squad_data(num_examples=10, num_segments=2)

@pytest.fixture
def gmm_training_config():
    """Minimal training config for tests."""
    return GMMTrainingConfig(
        use_gmm_memory=True,
        num_memory_experts=4,
        num_epochs=2,
        batch_size=2,
        learning_rate=1e-4,
    )
```

### Coverage Target Breakdown

**Target: >= 80% overall coverage**

| Module | Target Coverage | Priority |
|--------|-----------------|----------|
| memory_mixture.py | >= 90% | Critical |
| gating_network.py | >= 90% | Critical |
| expert_updates.py | >= 85% | High |
| memory_read.py | >= 85% | High |
| gmm_xlnet_qa.py | >= 80% | High |
| config.py | >= 80% | Medium |
| gmm_analysis.py | >= 70% | Medium (visualization) |

### Key Test Categories

**1. Unit Tests (Fast, < 1s each):**
- Test individual components in isolation
- Use small tensor sizes for speed
- Mock dependencies where possible
- Focus on correctness of core logic

**2. Integration Tests (Slow, 1-60s each):**
- Test full workflows end-to-end
- Use realistic (but minimal) data
- Verify component interactions
- Mark with `@pytest.mark.integration` and `@pytest.mark.slow`

**3. Regression Tests (Fast-Medium):**
- Verify existing functionality unchanged
- Compare GMM-disabled vs baseline
- No new functionality, only preservation
- Mark with `@pytest.mark.regression`

### Integration Test Example

```python
@pytest.mark.integration
@pytest.mark.slow
def test_gmm_training_loop(toy_gmm_model, toy_dataset, gmm_training_config):
    """Test full GMM training loop on toy data."""

    # Initialize trainer
    trainer = GMMTrainer(toy_gmm_model, gmm_training_config)

    # Get initial loss
    initial_loss = trainer.evaluate(toy_dataset)

    # Train for 2 epochs
    trainer.train(toy_dataset, num_epochs=2)

    # Get final loss
    final_loss = trainer.evaluate(toy_dataset)

    # Assert loss decreased
    assert final_loss < initial_loss, "Loss should decrease during training"

    # Assert routing probs valid
    routing_probs = trainer.get_routing_stats()
    assert torch.allclose(routing_probs.sum(dim=-1), torch.ones(1)), "Routing probs should sum to 1"
    assert not torch.isnan(routing_probs).any(), "Routing probs should not be NaN"

    # Assert checkpoint save/load works
    trainer.save_checkpoint("test_checkpoint")
    loaded_model = GMMXLNetForQA.from_pretrained("test_checkpoint")
    assert loaded_model.num_experts == 4, "Loaded model should have correct expert count"
```

### Regression Test Example

```python
@pytest.mark.regression
def test_existing_memxlnet_unchanged():
    """Verify existing MemXLNet functionality unchanged with GMM code present."""

    from memxlnet.models import MemXLNetForQA  # Original model
    from memxlnet.training import TrainingConfig  # Original config

    # Create baseline model (non-GMM)
    model = MemXLNetForQA.from_pretrained("xlnet-base-cased")

    # Run forward pass
    outputs = model(input_ids, attention_mask, memory_state)

    # Verify outputs have expected structure
    assert "start_logits" in outputs
    assert "end_logits" in outputs
    assert "new_memory_state" in outputs

    # Verify no GMM-specific keys present
    assert "routing_info" not in outputs
```

### Testing Standards

**pytest Configuration:**
```ini
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
markers = [
    "unit: Unit tests (fast)",
    "integration: Integration tests (slow)",
    "regression: Regression tests",
    "slow: Slow tests (>10s)",
]
addopts = [
    "--cov=src/gmmxlnet",
    "--cov-report=html",
    "--cov-report=term",
    "-v",
]
```

**Running Tests:**
```bash
# All tests
pytest tests/

# Unit tests only (fast)
pytest tests/unit/

# Integration tests
pytest tests/integration/ -m integration

# With coverage
pytest --cov=src/gmmxlnet --cov-report=html

# Parallel execution
pytest -n auto
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story created from PRD | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
