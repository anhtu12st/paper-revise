# Story 1.1: Core GMM Memory Expert Infrastructure

## Status

Draft

## Story

**As a** research engineer,
**I want** to implement the core multi-expert memory infrastructure,
**so that** the system can maintain multiple independent memory banks instead of a single monolithic memory state.

## Acceptance Criteria

1. **GatedMemoryMixture class created** in `src/gmmxlnet/models/memory_mixture.py` with support for k=2 to k=8 experts
2. **Independent expert initialization** with configurable per-expert initialization strategies (learned, zeros, uniform, orthogonal)
3. **Expert state management** methods: `get_expert_state(expert_idx)`, `set_expert_state(expert_idx, state)`, `reset_experts()`
4. **Shape validation** ensuring each expert has shape `(batch_size, memory_slots, hidden_dim)`
5. **Configuration parameters** added: `num_memory_experts`, `memory_expert_init_strategies`
6. **Unit tests** covering expert initialization, state access, and shape validation

## Integration Verification

**IV1**: Existing `DifferentiableMemory` class continues to pass all unit tests without modification
**IV2**: Existing `MemoryController` interface remains unchanged and functional
**IV3**: Module imports (`from memxlnet.models import MemXLNetForQA`) work without errors

## Tasks / Subtasks

- [ ] Create `src/gmmxlnet/` module structure (AC: 1)
  - [ ] Create `src/gmmxlnet/__init__.py`
  - [ ] Create `src/gmmxlnet/models/__init__.py`
  - [ ] Create `src/gmmxlnet/models/memory_mixture.py`
- [ ] Implement `GatedMemoryMixture` class (AC: 1, 2)
  - [ ] Define `__init__(num_experts, memory_slots, hidden_dim, init_strategies)` constructor
  - [ ] Implement learned initialization strategy
  - [ ] Implement zeros initialization strategy
  - [ ] Implement uniform initialization strategy
  - [ ] Implement orthogonal initialization strategy
  - [ ] Add configuration validation (num_experts in [2, 8])
- [ ] Implement expert state management methods (AC: 3)
  - [ ] Implement `get_expert_state(expert_idx)` method with bounds checking
  - [ ] Implement `set_expert_state(expert_idx, state)` method with shape validation
  - [ ] Implement `reset_experts()` method
  - [ ] Implement `get_all_experts()` batch access method
- [ ] Add shape validation (AC: 4)
  - [ ] Validate expert shapes in `forward()` method
  - [ ] Add assertions for (batch_size, memory_slots, hidden_dim)
  - [ ] Add clear error messages for shape mismatches
- [ ] Create unit tests (AC: 6)
  - [ ] Create `tests/unit/test_gmm_memory.py`
  - [ ] Test all initialization strategies
  - [ ] Test expert state access and modification
  - [ ] Test reset functionality
  - [ ] Test shape validation with various batch sizes
  - [ ] Test independent expert state management (no cross-contamination)
  - [ ] Verify >= 90% coverage for memory_mixture.py
- [ ] Run integration verification (IV1-IV3)
  - [ ] Run existing test suite: `pytest tests/unit/test_memory.py`
  - [ ] Verify `from memxlnet.models import MemXLNetForQA` imports work
  - [ ] Confirm zero modifications to existing memxlnet files

## Dev Notes

### Component Architecture

**GatedMemoryMixture Responsibility:**
- Manage k parallel memory expert banks with independent state tracking and initialization
- Provides unified interface for expert initialization, state access, and reset operations
- Integrates with existing memory token extraction logic

**Key Interfaces:**
- `__init__(num_experts, memory_slots, hidden_dim, init_strategies)` - Initialize k experts
- `get_expert_state(expert_idx)` → Tensor - Retrieve specific expert's memory
- `set_expert_state(expert_idx, state)` - Update specific expert's memory
- `reset_experts()` - Re-initialize all experts (for new document)
- `get_all_experts()` → List[Tensor] - Batch access to all expert states

**Technology Stack:**
- PyTorch `nn.Module` for learnable initialization parameters
- Shape: Each expert maintains `(batch_size, memory_slots, hidden_dim)` tensor
- Initialization: Supports learned, zeros, uniform, orthogonal strategies per expert

**Initialization Strategies (Implementation Guidance):**
```python
# Example initialization logic for each strategy
if init_strategy == "learned":
    expert_params = nn.Parameter(torch.randn(memory_slots, hidden_dim) * 0.02)
elif init_strategy == "zeros":
    expert_params = nn.Parameter(torch.zeros(memory_slots, hidden_dim))
elif init_strategy == "uniform":
    expert_params = nn.Parameter(torch.rand(memory_slots, hidden_dim) * 0.1)
elif init_strategy == "orthogonal":
    # Orthogonal initialization ensures linear independence between experts
    expert_params = nn.Parameter(torch.empty(memory_slots, hidden_dim))
    nn.init.orthogonal_(expert_params)
```

**Rationale:** Different strategies encourage different types of specialization. Orthogonal initialization is particularly useful for ensuring experts start with maximally different representations.

### Source Tree

**New Module Structure:**
```
src/gmmxlnet/                          # ✨ NEW GMM MODULE
├── __init__.py                        # Export GMMXLNetForQA, GMMConfig
└── models/
    ├── __init__.py                    # Export all GMM model components
    └── memory_mixture.py              # GatedMemoryMixture (k expert banks)
```

**Integration Pattern:**
- Parallel structure to `src/memxlnet/`
- Zero modifications to existing `src/memxlnet/` code
- GMM code imports from `memxlnet` as needed (one-way dependency)

### Coding Standards

**GMM-Specific Rules:**
- **Expert Indexing:** Always use 0-based indexing for experts (j ∈ [0, k-1]); include assertions for bounds checking
- **Memory Shapes:** All expert memories maintain identical shape (batch_size, memory_slots, hidden_dim); validate in forward pass
- **State Management:** Always return new memory states (never mutate in-place); facilitates time-step-major batching
- **Error Messages:** Include expert count and configuration in all GMM-specific error messages

**Naming Conventions:**
- Classes: PascalCase with GMM prefix (e.g., `GatedMemoryMixture`)
- Functions: snake_case (e.g., `get_expert_state`, `reset_experts`)
- Private methods: `_leading_underscore` (e.g., `_validate_expert_shapes`)
- Expert variables: `expert_*` (e.g., `expert_states`, `expert_params`)

**Configuration Validation:**
- Validate `num_experts` is in [2, 8] at initialization
- Fail fast with helpful error message for invalid configurations

### Testing

**Test File Location:**
- Create `tests/unit/test_gmm_memory.py`

**Testing Standards:**
- Framework: pytest 7.4.0+
- Naming: `test_<functionality>` functions
- Markers: Use `@pytest.mark.unit` for all tests
- Coverage Target: >= 90% (core component, critical for correctness)

**Test Requirements:**
- Test all initialization strategies (learned, zeros, uniform, orthogonal)
- Test expert state access and modification with bounds checking
- Test reset functionality
- Test shape validation for different batch sizes and expert counts (k=2, 4, 8)
- Test independent expert state management (updates don't cross-contaminate)

**Regression Testing:**
- All existing tests in `tests/unit/test_memory.py` must pass unchanged
- No modifications to existing MemXLNet tests

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Initial story created from PRD | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
