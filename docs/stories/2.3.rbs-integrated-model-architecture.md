# Story 2.3: RBS-QA Integrated Model Architecture

## Status
ðŸš§ **Approved**

## Acceptance Criteria
- [ ] Implement `RBSXLNetForQA` main model class integrating all RBS components
- [ ] Support both RBS mode and legacy GMM mode for backward compatibility
- [ ] Adaptive inference pipeline with segment-by-segment processing
- [ ] Complete integration of belief state tracking and halting policy
- [] Seamless model loading/saving with RBS state

## Description

This story implements the main **RBS-XLNetForQA** model that orchestrates all RBS-QA components (GMM backbone, belief state tracker, halting policy) into a unified architecture. The model supports both adaptive RBS mode and traditional GMM mode, ensuring complete backward compatibility while enabling the new experimental features.

## Implementation Details

### 1. RBS-XLNetForQA Main Model

**File Location**: `src/rbsqa/models/rbs_xlnet.py`

```python
class RBSXLNetForQA(nn.Module):
    """
    Recurrent Belief-State QA Model (RBS-QA)

    Integrates:
    - GMM-XLNet backbone with multi-expert memory
    - Dynamic Belief-State Tracker for non-monotonic reasoning
    - Halting Policy Network for adaptive computation
    """

    def __init__(self,
                 base_model_name: str = "xlnet-base-cased",
                 memory_num_tokens: int = 16,
                 num_memory_experts: int = 4,
                 use_rbs_mode: bool = True,
                 belief_state_config: Optional[Dict] = None,
                 halting_config: Optional[Dict] = None,
                 **kwargs):
        super().__init__()

        self.config = RBSModelConfig(
            base_model_name=base_model_name,
            memory_num_tokens=memory_num_tokens,
            num_memory_experts=num_memory_experts,
            use_rbs_mode=use_rbs_mode,
            **kwargs
        )

        # Initialize GMM-XLNet backbone
        self.gmm_backbone = GMMXLNetForQA(
            base_model_name=base_model_name,
            memory_num_tokens=memory_num_tokens,
            num_memory_experts=num_memory_experts,
            **kwargs
        )

        # RBS-specific components
        if use_rbs_mode:
            self.belief_tracker = BeliefStateTracker(
                **(belief_state_config or {})
            )
            self.halting_policy = HaltingPolicyNetwork(
                **(halting_config or {})
            )
        else:
            self.belief_tracker = None
            self.halting_policy = None

        # Mode management
        self.training_mode = "supervised"  # "supervised" or "rl"
        self.inference_mode = "adaptive"    # "adaptive" or "full"

    def forward(self,
                input_ids: torch.Tensor,
                attention_mask: torch.Tensor,
                memory_state: Optional[Dict[str, torch.Tensor]] = None,
                segment_info: Optional[Dict] = None,
                return_dict: bool = True,
                **kwargs) -> Union[Tuple, RBSModelOutput]:
        """
        Forward pass supporting both RBS and legacy modes.

        Args:
            input_ids: Token IDs [batch_size, seq_len]
            attention_mask: Attention mask [batch_size, seq_len]
            memory_state: Previous GMM memory state (for recurrent processing)
            segment_info: Information about current segment (id, offset, etc.)
            return_dict: Whether to return structured output

        Returns:
            Model outputs with QA logits, memory state, and RBS-specific info
        """

        if self.config.use_rbs_mode and segment_info is not None:
            return self._rbs_forward(
                input_ids, attention_mask, memory_state, segment_info, **kwargs
            )
        else:
            return self._legacy_forward(
                input_ids, attention_mask, memory_state, **kwargs
            )

    def _rbs_forward(self,
                    input_ids: torch.Tensor,
                    attention_mask: torch.Tensor,
                    memory_state: Optional[Dict[str, torch.Tensor]],
                    segment_info: Dict,
                    **kwargs) -> RBSModelOutput:
        """RBS mode forward pass with belief tracking and halting decisions."""

        # Forward through GMM backbone
        gmm_outputs = self.gmm_backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            memory_state=memory_state,
            return_dict=True,
            **kwargs
        )

        # Update belief state with current segment
        if self.belief_tracker is not None:
            updated_belief = self.belief_tracker.update_belief(
                current_logits=(gmm_outputs.start_logits, gmm_outputs.end_logits),
                current_segment_id=segment_info.get('segment_id', 0),
                gmm_context=gmm_outputs.aggregated_memory,
                global_offset=segment_info.get('global_offset', 0)
            )
        else:
            updated_belief = None

        # Extract halting features and make decision
        halting_decision = None
        if self.halting_policy is not None and self.training_mode == "rl":
            halting_features = self._extract_halting_features(
                updated_belief, gmm_outputs, segment_info
            )
            action, log_prob, value = self.halting_policy.select_action(
                halting_features, training=self.training
            )
            halting_decision = HaltingDecision(
                action=action,
                log_prob=log_prob,
                value_estimate=value,
                features=halting_features
            )

        return RBSModelOutput(
            start_logits=gmm_outputs.start_logits,
            end_logits=gmm_outputs.end_logits,
            memory_state=gmm_outputs.memory_state,
            aggregated_memory=gmm_outputs.aggregated_memory,
            routing_info=gmm_outputs.routing_info,
            belief_state=updated_belief,
            halting_decision=halting_decision,
            segment_info=segment_info,
            hidden_states=gmm_outputs.hidden_states,
            attentions=gmm_outputs.attentions
        )

    def _legacy_forward(self,
                       input_ids: torch.Tensor,
                       attention_mask: torch.Tensor,
                       memory_state: Optional[Dict[str, torch.Tensor]],
                       **kwargs) -> RBSModelOutput:
        """Legacy GMM mode forward pass (backward compatibility)."""

        gmm_outputs = self.gmm_backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            memory_state=memory_state,
            return_dict=True,
            **kwargs
        )

        return RBSModelOutput(
            start_logits=gmm_outputs.start_logits,
            end_logits=gmm_outputs.end_logits,
            memory_state=gmm_outputs.memory_state,
            aggregated_memory=gmm_outputs.aggregated_memory,
            routing_info=gmm_outputs.routing_info,
            belief_state=None,
            halting_decision=None,
            segment_info=None,
            hidden_states=gmm_outputs.hidden_states,
            attentions=gmm_outputs.attentions
        )

    def adaptive_inference(self,
                          question_input_ids: torch.Tensor,
                          context_segments: List[torch.Tensor],
                          max_segments: Optional[int] = None,
                          **kwargs) -> RBSInferenceResult:
        """
        Adaptive inference with early stopping based on confidence.

        Args:
            question_input_ids: Tokenized question [batch_size, question_len]
            context_segments: List of context segment token IDs
            max_segments: Maximum segments to process (None = all)

        Returns:
            Complete inference result with final answer and statistics
        """

        if not self.config.use_rbs_mode or self.inference_mode == "full":
            # Fall back to full processing
            return self._full_inference(question_input_ids, context_segments, **kwargs)

        # Initialize
        memory_state = None
        self.belief_tracker.reset_belief()

        processed_segments = []
        halting_history = []
        belief_history = []

        for segment_idx, segment_ids in enumerate(context_segments):
            if max_segments and segment_idx >= max_segments:
                break

            # Combine question and current segment
            input_ids = torch.cat([question_input_ids, segment_ids], dim=-1)
            attention_mask = torch.ones_like(input_ids)

            segment_info = {
                'segment_id': segment_idx,
                'global_offset': sum(s.size(-1) for s in context_segments[:segment_idx]),
                'is_last_segment': segment_idx == len(context_segments) - 1
            }

            # Forward pass
            outputs = self.forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                memory_state=memory_state,
                segment_info=segment_info,
                **kwargs
            )

            # Update memory state
            memory_state = outputs.memory_state

            # Store results
            processed_segments.append({
                'segment_id': segment_idx,
                'belief_state': outputs.belief_state,
                'start_logits': outputs.start_logits,
                'end_logits': outputs.end_logits
            })

            if outputs.belief_state:
                belief_history.append(outputs.belief_state)

            # Halting decision
            if outputs.halting_decision:
                halting_history.append(outputs.halting_decision)

                if outputs.halting_decision.action == "HALT":
                    break

        # Extract final answer
        final_belief = belief_history[-1] if belief_history else None
        if final_belief and final_belief.best_span:
            final_span = final_belief.best_span
            final_confidence = final_belief.confidence
        else:
            # Fallback: extract best from all processed segments
            final_span, final_confidence = self._extract_best_overall_span(processed_segments)

        return RBSInferenceResult(
            answer_span=final_span,
            confidence=final_confidence,
            segments_processed=len(processed_segments),
            total_segments=len(context_segments),
            belief_history=belief_history,
            halting_history=halting_history,
            memory_state=memory_state,
            efficiency_score=len(context_segments) / max(len(processed_segments), 1)
        )

    def _extract_halting_features(self,
                                 belief_state: BeliefState,
                                 gmm_outputs: Any,
                                 segment_info: Dict) -> HaltingStateFeatures:
        """Extract features for halting policy decision."""

        # Compute routing entropy
        routing_probs = gmm_outputs.routing_info.routing_probs
        routing_entropy = -torch.sum(routing_probs * torch.log(routing_probs + 1e-8), dim=-1).mean()

        # Expert utilization patterns
        expert_activation = (routing_probs > 0.1).float().mean(dim=0).cpu().tolist()

        return HaltingStateFeatures(
            current_confidence=belief_state.confidence if belief_state else 0.0,
            confidence_trend=getattr(belief_state, 'confidence_history', [])[-3:],
            confidence_variance=np.var(getattr(belief_state, 'confidence_history', [])),
            revision_count=getattr(belief_state, 'revision_count', 0),
            segments_processed=segment_info.get('segment_id', 0) + 1,
            segments_remaining=getattr(segment_info, 'total_segments', 10) - segment_info.get('segment_id', 0),
            processing_time=time.time(),  # Could be more sophisticated
            routing_entropy=routing_entropy.item(),
            expert_utilization=expert_activation,
            context_quality_score=self._compute_context_quality(gmm_outputs.aggregated_memory),
            document_length=getattr(segment_info, 'total_segments', 10),
            question_complexity=0.5,  # Could compute from question embedding
            segment_relevance_score=self._compute_segment_relevance(gmm_outputs)
        )

    def _compute_context_quality(self, aggregated_memory: torch.Tensor) -> float:
        """Compute quality score for aggregated GMM memory."""
        # Simple heuristic: lower variance in memory indicates better coherence
        memory_variance = torch.var(aggregated_memory, dim=-1).mean()
        return float(1.0 / (1.0 + memory_variance.item()))

    def _compute_segment_relevance(self, gmm_outputs: Any) -> float:
        """Compute relevance score for current segment."""
        # Use routing entropy as proxy for relevance
        routing_probs = gmm_outputs.routing_info.routing_probs
        max_prob = torch.max(routing_probs, dim=-1)[0].mean()
        return float(max_prob.item())

    def _extract_best_overall_span(self, processed_segments: List[Dict]) -> Tuple[Tuple[int, int], float]:
        """Extract best span from all processed segments."""
        best_span = (0, 0)
        best_confidence = 0.0

        for segment in processed_segments:
            if segment['belief_state'] and segment['belief_state'].confidence > best_confidence:
                best_span = segment['belief_state'].best_span
                best_confidence = segment['belief_state'].confidence

        return best_span, best_confidence

    def set_training_mode(self, mode: str) -> None:
        """Set training mode: 'supervised' or 'rl'."""
        if mode not in ["supervised", "rl"]:
            raise ValueError("Training mode must be 'supervised' or 'rl'")
        self.training_mode = mode

    def set_inference_mode(self, mode: str) -> None:
        """Set inference mode: 'adaptive' or 'full'."""
        if mode not in ["adaptive", "full"]:
            raise ValueError("Inference mode must be 'adaptive' or 'full'")
        self.inference_mode = mode

    def save_pretrained(self, save_directory: str, **kwargs) -> None:
        """Save model with RBS state."""
        os.makedirs(save_directory, exist_ok=True)

        # Save GMM backbone
        self.gmm_backbone.save_pretrained(os.path.join(save_directory, "gmm_backbone"))

        # Save RBS components
        if self.belief_tracker:
            torch.save(
                self.belief_tracker.state_dict(),
                os.path.join(save_directory, "belief_tracker.pt")
            )

        if self.halting_policy:
            torch.save(
                self.halting_policy.state_dict(),
                os.path.join(save_directory, "halting_policy.pt")
            )

        # Save config
        config_dict = asdict(self.config)
        with open(os.path.join(save_directory, "rbs_config.json"), "w") as f:
            json.dump(config_dict, f, indent=2)

    @classmethod
    def from_pretrained(cls, model_path: str, **kwargs) -> "RBSXLNetForQA":
        """Load model from checkpoint."""
        # Load config
        config_path = os.path.join(model_path, "rbs_config.json")
        if os.path.exists(config_path):
            with open(config_path, "r") as f:
                config_dict = json.load(f)
            config = RBSModelConfig(**config_dict)
        else:
            # Fallback to default config for legacy models
            config = RBSModelConfig()

        # Initialize model
        model = cls(
            base_model_name=config.base_model_name,
            memory_num_tokens=config.memory_num_tokens,
            num_memory_experts=config.num_memory_experts,
            use_rbs_mode=config.use_rbs_mode,
            **kwargs
        )

        # Load GMM backbone
        gmm_path = os.path.join(model_path, "gmm_backbone")
        if os.path.exists(gmm_path):
            model.gmm_backbone = GMMXLNetForQA.from_pretrained(gmm_path)

        # Load RBS components
        belief_path = os.path.join(model_path, "belief_tracker.pt")
        if os.path.exists(belief_path) and model.belief_tracker:
            model.belief_tracker.load_state_dict(torch.load(belief_path))

        halting_path = os.path.join(model_path, "halting_policy.pt")
        if os.path.exists(halting_path) and model.halting_policy:
            model.halting_policy.load_state_dict(torch.load(halting_path))

        return model

    def get_memory_state(self) -> Dict[str, torch.Tensor]:
        """Get current memory state for propagation."""
        return self.gmm_backbone.get_memory_state()

    def set_memory_state(self, memory_state: Dict[str, torch.Tensor]) -> None:
        """Set memory state for continued processing."""
        self.gmm_backbone.set_memory_state(memory_state)


@dataclass
class RBSModelOutput:
    """Structured output for RBS model."""
    start_logits: torch.Tensor
    end_logits: torch.Tensor
    memory_state: Dict[str, torch.Tensor]
    aggregated_memory: torch.Tensor
    routing_info: Any
    belief_state: Optional[BeliefState]
    halting_decision: Optional[HaltingDecision]
    segment_info: Optional[Dict]
    hidden_states: Optional[Tuple[torch.Tensor]]
    attentions: Optional[Tuple[torch.Tensor]]


@dataclass
class RBSInferenceResult:
    """Result of adaptive inference."""
    answer_span: Tuple[int, int]
    confidence: float
    segments_processed: int
    total_segments: int
    belief_history: List[BeliefState]
    halting_history: List[HaltingDecision]
    memory_state: Dict[str, torch.Tensor]
    efficiency_score: float  # Higher is better (more efficient)


@dataclass
class HaltingDecision:
    """Halting policy decision."""
    action: str  # "CONTINUE" or "HALT"
    log_prob: torch.Tensor
    value_estimate: torch.Tensor
    features: HaltingStateFeatures


@dataclass
class RBSModelConfig:
    """Configuration for RBS model."""
    base_model_name: str = "xlnet-base-cased"
    memory_num_tokens: int = 16
    num_memory_experts: int = 4
    use_rbs_mode: bool = True
    hidden_dropout_prob: float = 0.1
    attention_probs_dropout_prob: float = 0.1
    initializer_range: float = 0.02
    layer_norm_eps: float = 1e-12
```

### 2. RBS Training Pipeline Integration

**File Location**: `src/rbsqa/training/rbs_trainer.py`

```python
class RBSTrainer:
    """Unified trainer for both supervised and RL phases of RBS training."""

    def __init__(self,
                 model: RBSXLNetForQA,
                 config: RBSTrainingConfig,
                 train_dataloader: DataLoader,
                 eval_dataloader: Optional[DataLoader] = None):

        self.model = model
        self.config = config
        self.train_dataloader = train_dataloader
        self.eval_dataloader = eval_dataloader

        # Optimizers
        self.qa_optimizer = torch.optim.AdamW(
            model.parameters(), lr=config.learning_rate
        )

        if config.use_rbs_mode and config.use_rl_training:
            self.rl_optimizer = torch.optim.Adam(
                model.halting_policy.parameters(), lr=config.rl_learning_rate
            )

        # Training state
        self.current_epoch = 0
        self.training_stage = "supervised"  # "supervised" or "rl"

    def train(self, num_epochs: int) -> Dict[str, List[float]]:
        """Main training loop with stage transitions."""
        training_history = {
            'qa_loss': [],
            'rl_loss': [],
            'eval_f1': [],
            'efficiency_score': []
        }

        for epoch in range(num_epochs):
            self.current_epoch = epoch

            # Stage transition logic
            if epoch == self.config.rl_start_epoch and self.training_stage == "supervised":
                self.training_stage = "rl"
                self.model.set_training_mode("rl")
                print(f"Transitioning to RL training at epoch {epoch}")

            # Train epoch
            epoch_metrics = self.train_epoch()

            # Update history
            for key, value in epoch_metrics.items():
                if key in training_history:
                    training_history[key].append(value)

            # Evaluation
            if self.eval_dataloader and (epoch + 1) % self.config.eval_frequency == 0:
                eval_metrics = self.evaluate()
                training_history['eval_f1'].append(eval_metrics['f1'])
                training_history['efficiency_score'].append(eval_metrics['efficiency_score'])

                print(f"Epoch {epoch}: Eval F1={eval_metrics['f1']:.3f}, "
                      f"Efficiency={eval_metrics['efficiency_score']:.3f}")

        return training_history

    def train_epoch(self) -> Dict[str, float]:
        """Train one epoch with appropriate loss computation."""
        if self.training_stage == "supervised":
            return self._supervised_epoch()
        else:
            return self._rl_epoch()

    def _supervised_epoch(self) -> Dict[str, float]:
        """Supervised training phase (Stage 1)."""
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        for batch in self.train_dataloader:
            self.qa_optimizer.zero_grad()

            # Forward pass (no adaptive processing during supervised training)
            outputs = self.model.forward(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                memory_state=batch.get('memory_state'),
                return_dict=True
            )

            # Compute QA loss
            loss = self._compute_qa_loss(
                outputs.start_logits,
                outputs.end_logits,
                batch['start_positions'],
                batch['end_positions']
            )

            # Backward pass
            loss.backward()
            self.qa_optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        return {
            'qa_loss': total_loss / max(num_batches, 1),
            'rl_loss': 0.0
        }

    def _rl_epoch(self) -> Dict[str, float]:
        """RL training phase (Stage 2)."""
        self.model.train()
        total_qa_loss = 0.0
        total_rl_loss = 0.0
        num_episodes = 0

        episodes_data = []
        ground_truth_spans = []

        for batch in self.train_dataloader:
            # Process document adaptively to collect episodes
            episode_data, gt_span = self._collect_rl_episode(batch)

            if episode_data:
                episodes_data.append(episode_data)
                ground_truth_spans.append(gt_span)
                num_episodes += 1

            # Accumulate episodes for RL update
            if len(episodes_data) >= self.config.rl_batch_size:
                rl_loss = self._update_rl_policy(episodes_data, ground_truth_spans)
                total_rl_loss += rl_loss.item()
                episodes_data = []
                ground_truth_spans = []

        # Process remaining episodes
        if episodes_data:
            rl_loss = self._update_rl_policy(episodes_data, ground_truth_spans)
            total_rl_loss += rl_loss.item()

        return {
            'qa_loss': total_qa_loss / max(len(self.train_dataloader), 1),
            'rl_loss': total_rl_loss / max(num_episodes, 1)
        }

    def _collect_rl_episode(self, batch: Dict) -> Tuple[List[Dict], Tuple[int, int]]:
        """Collect one RL episode by processing document adaptively."""
        episode_steps = []

        # Initialize
        memory_state = batch.get('memory_state')
        self.model.belief_tracker.reset_belief()

        # Process segments adaptively
        for segment_idx in range(batch['num_segments']):
            segment_input_ids = batch['input_ids'][:, segment_idx]
            segment_attention_mask = batch['attention_mask'][:, segment_idx]

            segment_info = {
                'segment_id': segment_idx,
                'global_offset': batch['segment_offsets'][segment_idx],
                'total_segments': batch['num_segments']
            }

            # Forward pass
            outputs = self.model.forward(
                input_ids=segment_input_ids,
                attention_mask=segment_attention_mask,
                memory_state=memory_state,
                segment_info=segment_info,
                return_dict=True
            )

            # Update memory state
            memory_state = outputs.memory_state

            # Store episode step
            episode_steps.append({
                'outputs': outputs,
                'belief_state': outputs.belief_state,
                'halting_decision': outputs.halting_decision,
                'segment_info': segment_info
            })

            # Check halting decision
            if outputs.halting_decision and outputs.halting_decision.action == "HALT":
                break

        # Ground truth span
        gt_span = (
            batch['global_start_positions'][0].item(),
            batch['global_end_positions'][0].item()
        )

        return episode_steps, gt_span

    def _update_rl_policy(self,
                         episodes_data: List[List[Dict]],
                         ground_truth_spans: List[Tuple[int, int]]) -> torch.Tensor:
        """Update halting policy using collected episodes."""

        # Compute rewards
        rewards = self.model.halting_policy.compute_rewards(
            episodes_data, ground_truth_spans, self.config.lambda_cost
        )

        # Extract episodes for policy update
        policy_episodes = []
        for episode in episodes_data:
            policy_episode = []
            for step in episode:
                if step['halting_decision']:
                    policy_episode.append({
                        'features': step['halting_decision'].features,
                        'action': step['halting_decision'].action,
                        'log_prob': step['halting_decision'].log_prob,
                        'value_estimate': step['halting_decision'].value_estimate,
                        'predicted_span': step['belief_state'].best_span if step['belief_state'] else (0, 0)
                    })
            policy_episodes.append(policy_episode)

        # Compute policy gradient loss
        policy_loss = self.model.halting_policy.policy_gradient_loss(
            policy_episodes, rewards, gamma=self.config.gamma
        )

        # Optional value loss
        total_rl_loss = policy_loss
        if self.config.use_value_baseline:
            value_loss = self.model.halting_policy.value_loss(
                policy_episodes, rewards, gamma=self.config.gamma
            )
            total_rl_loss = policy_loss + self.config.value_weight * value_loss

        # Update policy network
        self.rl_optimizer.zero_grad()
        total_rl_loss.backward()
        self.rl_optimizer.step()

        return total_rl_loss

    def evaluate(self) -> Dict[str, float]:
        """Evaluate model with adaptive inference."""
        self.model.eval()
        self.model.set_inference_mode("adaptive")

        total_f1 = 0.0
        total_efficiency = 0.0
        num_examples = 0

        with torch.no_grad():
            for batch in self.eval_dataloader:
                # Adaptive inference
                question_ids = batch['question_input_ids']
                context_segments = batch['context_segments']

                result = self.model.adaptive_inference(
                    question_input_ids=question_ids,
                    context_segments=context_segments
                )

                # Compute metrics
                gt_span = (
                    batch['global_start_positions'][0].item(),
                    batch['global_end_positions'][0].item()
                )

                f1 = self.model.halting_policy.compute_f1_score(
                    result.answer_span, gt_span
                )

                total_f1 += f1
                total_efficiency += result.efficiency_score
                num_examples += 1

        return {
            'f1': total_f1 / max(num_examples, 1),
            'efficiency_score': total_efficiency / max(num_examples, 1)
        }

    def _compute_qa_loss(self,
                        start_logits: torch.Tensor,
                        end_logits: torch.Tensor,
                        start_positions: torch.Tensor,
                        end_positions: torch.Tensor) -> torch.Tensor:
        """Compute standard QA loss."""
        start_loss = F.cross_entropy(start_logits, start_positions)
        end_loss = F.cross_entropy(end_logits, end_positions)
        return (start_loss + end_loss) / 2
```

### 3. Configuration Integration

**File Location**: `src/rbsqa/configs/rbs_config.py`

```python
@dataclass
class RBSTrainingConfig:
    # Base GMM configuration
    memory_num_tokens: int = 16
    num_memory_experts: int = 4
    learning_rate: float = 5e-5

    # RBS-specific settings
    use_rbs_mode: bool = True
    belief_state_threshold: float = 0.7

    # Belief state configuration
    belief_max_segments: int = 32
    belief_re_scoring_method: str = "context_weighted"
    belief_enable_trend_analysis: bool = True

    # Halting policy configuration
    halting_hidden_dim: int = 64
    halting_num_layers: int = 2
    halting_temperature: float = 1.0
    halting_exploration_rate: float = 0.1

    # RL training settings
    use_rl_training: bool = True
    rl_start_epoch: int = 2
    rl_weight: float = 0.1
    rl_learning_rate: float = 1e-4
    rl_batch_size: int = 8
    lambda_cost: float = 0.01
    gamma: float = 0.99
    use_value_baseline: bool = True
    value_weight: float = 0.5

    # General training settings
    num_epochs: int = 10
    eval_frequency: int = 1
    save_frequency: int = 2
    seed: int = 42

    @classmethod
    def rbs_balanced_config(cls, **kwargs) -> "RBSTrainingConfig":
        """Preset configuration for balanced RBS training."""
        return cls(
            memory_num_tokens=16,
            num_memory_experts=4,
            belief_state_threshold=0.7,
            halting_temperature=1.0,
            rl_weight=0.1,
            lambda_cost=0.01,
            **kwargs
        )

    @classmethod
    def rbs_efficiency_config(cls, **kwargs) -> "RBSTrainingConfig":
        """Preset configuration optimized for efficiency."""
        return cls(
            memory_num_tokens=12,
            num_memory_experts=2,
            belief_state_threshold=0.6,
            halting_temperature=0.8,
            rl_weight=0.2,
            lambda_cost=0.02,
            **kwargs
        )

    @classmethod
    def rbs_accuracy_config(cls, **kwargs) -> "RBSTrainingConfig":
        """Preset configuration optimized for accuracy."""
        return cls(
            memory_num_tokens=20,
            num_memory_experts=6,
            belief_state_threshold=0.8,
            halting_temperature=1.2,
            rl_weight=0.05,
            lambda_cost=0.005,
            **kwargs
        )
```

### 4. Backward Compatibility Strategy

**Legacy Model Loading**:
```python
def load_model_with_backward_compatibility(model_path: str, **kwargs) -> RBSXLNetForQA:
    """Load model with automatic backward compatibility detection."""

    # Check if it's an RBS model
    rbs_config_path = os.path.join(model_path, "rbs_config.json")
    if os.path.exists(rbs_config_path):
        # Load as RBS model
        return RBSXLNetForQA.from_pretrained(model_path, **kwargs)

    # Check if it's a GMM model
    gmm_config_path = os.path.join(model_path, "gmm_config.json")
    if os.path.exists(gmm_config_path):
        # Load GMM model and wrap in RBS interface
        gmm_model = GMMXLNetForQA.from_pretrained(model_path, **kwargs)
        return RBSXLNetForQA.wrap_gmm_model(gmm_model)

    # Check if it's base MemXLNet model
    base_config_path = os.path.join(model_path, "config.json")
    if os.path.exists(base_config_path):
        # Load base model and create minimal RBS wrapper
        base_model = MemXLNetForQA.from_pretrained(model_path, **kwargs)
        return RBSXLNetForQA.wrap_base_model(base_model)

    raise ValueError(f"No valid model found in {model_path}")
```

### 5. Testing Strategy

**Unit Tests**:
- `test_rbs_model_forward_rbs_mode`
- `test_rbs_model_forward_legacy_mode`
- `test_adaptive_inference_early_halt`
- `test_adaptive_inference_full_processing`
- `test_belief_state_integration`
- `test_halting_policy_integration`

**Integration Tests**:
- `test_backward_compatibility_gmm_loading`
- `test_backward_compatibility_base_loading`
- `test_training_stage_transitions`
- `test_model_serialization_deserialization`

**End-to-End Tests**:
- `test_full_rbs_training_pipeline`
- `test_adaptive_inference_accuracy_efficiency`
- `test_checkpoint_resume_training`

### 6. Success Metrics

**Functional Requirements**:
- 100% backward compatibility with existing models
- Seamless mode switching between RBS and legacy
- Adaptive inference reduces average segments processed by >30%
- No accuracy degradation >2% compared to full processing

**Performance Requirements**:
- Model loading time < 2 seconds
- Adaptive inference overhead < 10ms per segment
- Memory usage increase < 15% compared to GMM baseline
- Training stability across stage transitions

### 7. Dependencies

**Required Components**:
- Story 2.1: BeliefStateTracker
- Story 2.2: HaltingPolicyNetwork
- Existing GMM-XLNet components
- Base training infrastructure

**External Dependencies**:
- PyTorch >= 2.8.0
- Transformers >= 4.56.2
- NumPy, tqdm, json, os, time

## Definition of Done

- [ ] All unit tests pass (95%+ coverage)
- [ ] Integration tests verify backward compatibility
- [ ] End-to-end training pipeline produces stable results
- [ ] Adaptive inference demonstrates efficiency gains
- [ ] Model can be loaded/saved with RBS state
- [ ] Configuration system supports all presets
- [ ] Documentation updated with usage examples
- [ ] Performance benchmarks meet success criteria

## Out of Scope

- Distributed training optimizations
- Advanced RL algorithms (PPO, A2C, etc.)
- Multi-task learning extensions
- Real-time inference optimizations

## Notes

The RBS-XLNetForQA model serves as the central orchestrator that seamlessly integrates all RBS components while maintaining complete backward compatibility. Key design principles:

1. **Unified Interface**: Single model class supports both RBS and legacy modes
2. **Adaptive Execution**: Runtime decisions based on confidence and policy
3. **Stage-Aware Training**: Proper transitions between supervised and RL phases
4. **State Management**: Proper serialization of all RBS components
5. **Compatibility**: Zero-impact loading of existing models

The model should feel natural to use whether you're doing traditional QA or experimenting with the new RBS features.