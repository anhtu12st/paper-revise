# Story 2.4: RBS-QA Training Pipeline with Hybrid SL+RL Learning

## Status
ðŸš§ **Approved**

## Acceptance Criteria
- [ ] Implement hybrid training pipeline combining supervised QA and RL halting policy
- [ ] Support stage transitions: supervised pre-training â†’ hybrid fine-tuning
- [ ] Proper curriculum learning with progressive segment complexity
- [ ] Comprehensive evaluation metrics for accuracy and efficiency
- [ ] Training stability with checkpoint management and resumption

## Description

This story implements the complete **hybrid training pipeline** for RBS-QA that combines supervised learning for QA accuracy with reinforcement learning for computational efficiency. The pipeline supports the two-stage training approach described in the research: Stage 1 (supervised pre-training) and Stage 2 (hybrid SL+RL fine-tuning).

## Implementation Details

### 1. Hybrid Training Manager

**File Location**: `src/rbsqa/training/hybrid_trainer.py`

```python
class RBSHybridTrainer:
    """
    Hybrid trainer for RBS-QA combining supervised QA learning with RL halting policy training.

    Stage 1: Pure supervised pre-training of GMM backbone and belief state tracker
    Stage 2: Hybrid fine-tuning with combined SL (stability) + RL (efficiency) losses
    """

    def __init__(self,
                 model: RBSXLNetForQA,
                 config: RBSTrainingConfig,
                 train_dataset: Dataset,
                 eval_dataset: Optional[Dataset] = None,
                 resume_from_checkpoint: Optional[str] = None):

        self.model = model
        self.config = config
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset

        # Initialize data loaders
        self.train_loader = self._create_dataloader(train_dataset, shuffle=True)
        self.eval_loader = self._create_dataloader(eval_dataset, shuffle=False) if eval_dataset else None

        # Initialize optimizers
        self._setup_optimizers()

        # Training state
        self.current_epoch = 0
        self.global_step = 0
        self.training_stage = "supervised"  # "supervised" or "hybrid"
        self.best_eval_score = 0.0
        self.training_history = defaultdict(list)

        # Checkpoint management
        self.checkpoint_manager = CheckpointManager(
            save_dir=self.config.output_dir,
            save_frequency=self.config.save_frequency,
            keep_best=self.config.keep_best_checkpoints
        )

        # Logging
        self.logger = self._setup_logger()
        self.wandb_enabled = self._setup_wandb()

        # Resume from checkpoint if specified
        if resume_from_checkpoint:
            self.load_checkpoint(resume_from_checkpoint)

    def _setup_optimizers(self) -> None:
        """Setup separate optimizers for different components."""

        # QA optimizer (GMM backbone + belief state tracker)
        qa_params = []
        qa_params.extend(self.model.gmm_backbone.parameters())
        if self.model.belief_tracker:
            qa_params.extend(self.model.belief_tracker.parameters())

        self.qa_optimizer = torch.optim.AdamW(
            qa_params,
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            eps=self.config.adam_epsilon
        )

        # RL optimizer (halting policy network only)
        if self.model.halting_policy and self.config.use_rl_training:
            self.rl_optimizer = torch.optim.Adam(
                self.model.halting_policy.parameters(),
                lr=self.config.rl_learning_rate,
                weight_decay=self.config.rl_weight_decay
            )

        # Learning rate schedulers
        self.qa_scheduler = self._create_scheduler(self.qa_optimizer, "qa")
        if hasattr(self, 'rl_optimizer'):
            self.rl_scheduler = self._create_scheduler(self.rl_optimizer, "rl")

    def _create_dataloader(self, dataset: Dataset, shuffle: bool) -> DataLoader:
        """Create data loader with proper collation for RBS training."""

        def rbs_collate_fn(batch):
            """Custom collate function for RBS batch processing."""

            # Standard QA fields
            input_ids = torch.stack([item['input_ids'] for item in batch])
            attention_mask = torch.stack([item['attention_mask'] for item in batch])

            # Segment information for adaptive processing
            if 'segment_ids' in batch[0]:
                segment_ids = torch.stack([item['segment_ids'] for item in batch])
                segment_offsets = torch.stack([item['segment_offsets'] for item in batch])
                num_segments = torch.stack([item['num_segments'] for item in batch])
            else:
                # Legacy single-segment format
                segment_ids = input_ids.unsqueeze(1)  # Add segment dimension
                segment_offsets = torch.zeros(input_ids.size(0), 1, dtype=torch.long)
                num_segments = torch.ones(input_ids.size(0), dtype=torch.long)

            # Answer positions
            start_positions = torch.stack([item['start_positions'] for item in batch])
            end_positions = torch.stack([item['end_positions'] for item in batch])

            # Global positions for multi-segment documents
            if 'global_start_positions' in batch[0]:
                global_start_positions = torch.stack([item['global_start_positions'] for item in batch])
                global_end_positions = torch.stack([item['global_end_positions'] for item in batch])
            else:
                # Single-segment fallback
                global_start_positions = start_positions
                global_end_positions = end_positions

            # Question and context separation (for adaptive inference)
            question_input_ids = []
            context_segments = []

            for item in batch:
                if 'question_input_ids' in item:
                    question_input_ids.append(item['question_input_ids'])
                    context_segments.append(item['context_segments'])
                else:
                    # Split input_ids into question + context if not pre-separated
                    sep_pos = item.get('question_length', item['input_ids'].size(0) // 2)
                    question_input_ids.append(item['input_ids'][:sep_pos])
                    context_segments.append([item['input_ids'][sep_pos:]])

            return {
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'segment_ids': segment_ids,
                'segment_offsets': segment_offsets,
                'num_segments': num_segments,
                'start_positions': start_positions,
                'end_positions': end_positions,
                'global_start_positions': global_start_positions,
                'global_end_positions': global_end_positions,
                'question_input_ids': question_input_ids,
                'context_segments': context_segments
            }

        return DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=shuffle,
            collate_fn=rbs_collate_fn,
            num_workers=self.config.dataloader_num_workers,
            pin_memory=self.config.dataloader_pin_memory
        )

    def train(self) -> Dict[str, Any]:
        """Main training loop with automatic stage transitions."""

        self.logger.info("Starting RBS-QA hybrid training...")
        self.logger.info(f"Configuration: {json.dumps(asdict(self.config), indent=2)}")

        total_training_time = 0
        training_start_time = time.time()

        try:
            for epoch in range(self.current_epoch, self.config.num_epochs):
                epoch_start_time = time.time()
                self.current_epoch = epoch

                # Check for stage transition
                self._check_stage_transition()

                # Train one epoch
                epoch_metrics = self.train_epoch()
                epoch_time = time.time() - epoch_start_time
                total_training_time += epoch_time

                # Update learning rate schedulers
                self.qa_scheduler.step()
                if hasattr(self, 'rl_scheduler'):
                    self.rl_scheduler.step()

                # Evaluation
                if self.eval_loader and (epoch + 1) % self.config.eval_frequency == 0:
                    eval_metrics = self.evaluate()
                    self._log_eval_metrics(epoch, eval_metrics)

                    # Check for best model
                    current_score = eval_metrics.get('combined_score', eval_metrics.get('f1', 0.0))
                    if current_score > self.best_eval_score:
                        self.best_eval_score = current_score
                        self.save_checkpoint(f"best-model")
                        self.logger.info(f"New best model: {current_score:.4f}")

                # Log epoch metrics
                self._log_epoch_metrics(epoch, epoch_metrics, eval_metrics if self.eval_loader else None)

                # Save checkpoint
                if (epoch + 1) % self.config.save_frequency == 0:
                    self.save_checkpoint(f"epoch-{epoch + 1}")

                # Early stopping
                if self._should_early_stop():
                    self.logger.info("Early stopping triggered")
                    break

        except KeyboardInterrupt:
            self.logger.info("Training interrupted by user")
            self.save_checkpoint("interrupted")
        except Exception as e:
            self.logger.error(f"Training failed: {str(e)}")
            self.save_checkpoint("failed")
            raise

        total_time = time.time() - training_start_time
        self.logger.info(f"Training completed in {total_time:.2f} seconds")

        return {
            'total_time': total_time,
            'best_score': self.best_eval_score,
            'final_epoch': self.current_epoch,
            'training_history': dict(self.training_history)
        }

    def train_epoch(self) -> Dict[str, float]:
        """Train one epoch with appropriate loss computation."""

        self.model.train()
        epoch_metrics = defaultdict(float)
        num_batches = 0

        # RL episode collection (only in hybrid stage)
        rl_episodes = []
        rl_ground_truths = []

        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc=f"Epoch {self.current_epoch}")):

            if self.training_stage == "supervised":
                batch_metrics = self._supervised_training_step(batch)
            else:
                batch_metrics, episode_data = self._hybrid_training_step(batch)

                if episode_data:
                    rl_episodes.extend(episode_data['episodes'])
                    rl_ground_truths.extend(episode_data['ground_truths'])

            # Accumulate metrics
            for key, value in batch_metrics.items():
                epoch_metrics[key] += value

            num_batches += 1
            self.global_step += 1

            # Log batch metrics (less frequently)
            if self.global_step % self.config.logging_steps == 0:
                self._log_batch_metrics(batch_metrics, batch_idx)

        # Process collected RL episodes
        if self.training_stage == "hybrid" and rl_episodes:
            rl_metrics = self._process_rl_episodes(rl_episodes, rl_ground_truths)
            for key, value in rl_metrics.items():
                epoch_metrics[key] += value

        # Average metrics
        return {key: value / max(num_batches, 1) for key, value in epoch_metrics.items()}

    def _supervised_training_step(self, batch: Dict) -> Dict[str, float]:
        """Supervised training step (Stage 1)."""

        self.qa_optimizer.zero_grad()

        # Forward pass through all segments (full document processing)
        total_qa_loss = 0.0
        memory_state = None

        num_segments = batch['num_segments'][0].item()

        for segment_idx in range(num_segments):
            # Extract segment data
            segment_input_ids = batch['input_ids'][:, segment_idx]
            segment_attention_mask = batch['attention_mask'][:, segment_idx]

            # Forward pass (legacy mode)
            outputs = self.model.forward(
                input_ids=segment_input_ids,
                attention_mask=segment_attention_mask,
                memory_state=memory_state,
                return_dict=True
            )

            # Compute QA loss for this segment
            qa_loss = self._compute_qa_loss(
                outputs.start_logits,
                outputs.end_logits,
                batch['start_positions'][:, segment_idx],
                batch['end_positions'][:, segment_idx]
            )

            total_qa_loss += qa_loss
            memory_state = outputs.memory_state

        # Average loss across segments
        avg_qa_loss = total_qa_loss / num_segments

        # Backward pass
        avg_qa_loss.backward()

        # Gradient clipping
        if self.config.max_grad_norm > 0:
            torch.nn.utils.clip_grad_norm_(
                list(self.model.gmm_backbone.parameters()) +
                (list(self.model.belief_tracker.parameters()) if self.model.belief_tracker else []),
                self.config.max_grad_norm
            )

        self.qa_optimizer.step()

        return {'qa_loss': avg_qa_loss.item()}

    def _hybrid_training_step(self, batch: Dict) -> Tuple[Dict[str, float], Optional[Dict]]:
        """Hybrid training step combining SL and RL (Stage 2)."""

        # QA loss computation (stability component)
        self.qa_optimizer.zero_grad()

        total_qa_loss = 0.0
        memory_state = None

        # Process all segments for QA loss (full document)
        num_segments = batch['num_segments'][0].item()

        for segment_idx in range(num_segments):
            segment_input_ids = batch['input_ids'][:, segment_idx]
            segment_attention_mask = batch['attention_mask'][:, segment_idx]

            outputs = self.model.forward(
                input_ids=segment_input_ids,
                attention_mask=segment_attention_mask,
                memory_state=memory_state,
                return_dict=True
            )

            qa_loss = self._compute_qa_loss(
                outputs.start_logits,
                outputs.end_logits,
                batch['start_positions'][:, segment_idx],
                batch['end_positions'][:, segment_idx]
            )

            total_qa_loss += qa_loss
            memory_state = outputs.memory_state

        avg_qa_loss = total_qa_loss / num_segments

        # Backward pass for QA loss
        avg_qa_loss.backward(retain_graph=True)

        # RL episode collection
        episode_data = self._collect_rl_episode(batch)

        # Update QA parameters
        if self.config.max_grad_norm > 0:
            torch.nn.utils.clip_grad_norm_(
                list(self.model.gmm_backbone.parameters()) +
                (list(self.model.belief_tracker.parameters()) if self.model.belief_tracker else []),
                self.config.max_grad_norm
            )

        self.qa_optimizer.step()

        return {'qa_loss': avg_qa_loss.item()}, episode_data

    def _collect_rl_episode(self, batch: Dict) -> Optional[Dict]:
        """Collect RL episodes by processing documents adaptively."""

        episodes = []
        ground_truths = []

        for batch_idx in range(batch['input_ids'].size(0)):
            # Reset belief state for new document
            self.model.belief_tracker.reset_belief()
            memory_state = None

            episode_steps = []
            document_segments = batch['context_segments'][batch_idx]
            question_ids = batch['question_input_ids'][batch_idx]

            # Process segments adaptively
            for segment_idx, segment_ids in enumerate(document_segments):
                # Combine question and current segment
                input_ids = torch.cat([question_ids.unsqueeze(0), segment_ids.unsqueeze(0)], dim=-1)
                attention_mask = torch.ones_like(input_ids)

                segment_info = {
                    'segment_id': segment_idx,
                    'global_offset': sum(s.size(-1) for s in document_segments[:segment_idx]),
                    'total_segments': len(document_segments)
                }

                # Forward pass in RBS mode
                self.model.eval()  # Set to eval for episode collection
                with torch.no_grad():
                    outputs = self.model.forward(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        memory_state=memory_state,
                        segment_info=segment_info,
                        return_dict=True
                    )
                self.model.train()

                # Store episode step
                if outputs.halting_decision:
                    episode_steps.append({
                        'features': outputs.halting_decision.features,
                        'action': outputs.halting_decision.action,
                        'log_prob': outputs.halting_decision.log_prob,
                        'value_estimate': outputs.halting_policy.value_estimate,
                        'predicted_span': outputs.belief_state.best_span if outputs.belief_state else (0, 0)
                    })

                    # Check halting decision
                    if outputs.halting_decision.action == "HALT":
                        break

                memory_state = outputs.memory_state

            # Store episode and ground truth
            if episode_steps:
                episodes.append(episode_steps)
                ground_truths.append((
                    batch['global_start_positions'][batch_idx].item(),
                    batch['global_end_positions'][batch_idx].item()
                ))

        if episodes:
            return {'episodes': episodes, 'ground_truths': ground_truths}
        else:
            return None

    def _process_rl_episodes(self,
                           episodes: List[List[Dict]],
                           ground_truths: List[Tuple[int, int]]) -> Dict[str, float]:
        """Process collected RL episodes and update halting policy."""

        if not episodes or not self.model.halting_policy:
            return {'rl_loss': 0.0}

        self.rl_optimizer.zero_grad()

        # Compute rewards
        rewards = self.model.halting_policy.compute_rewards(
            episodes, ground_truths, self.config.lambda_cost
        )

        # Compute policy gradient loss
        policy_loss = self.model.halting_policy.policy_gradient_loss(
            episodes, rewards, gamma=self.config.gamma
        )

        # Optional value loss
        total_rl_loss = policy_loss
        if self.config.use_value_baseline:
            value_loss = self.model.halting_policy.value_loss(
                episodes, rewards, gamma=self.config.gamma
            )
            total_rl_loss = policy_loss + self.config.value_weight * value_loss

        # Backward pass
        total_rl_loss.backward()

        if self.config.max_grad_norm > 0:
            torch.nn.utils.clip_grad_norm_(
                self.model.halting_policy.parameters(),
                self.config.max_grad_norm
            )

        self.rl_optimizer.step()

        return {
            'rl_loss': total_rl_loss.item(),
            'avg_reward': np.mean([np.mean(episode_rewards) for episode_rewards in rewards]),
            'avg_episode_length': np.mean([len(episode) for episode in episodes])
        }

    def _compute_qa_loss(self,
                        start_logits: torch.Tensor,
                        end_logits: torch.Tensor,
                        start_positions: torch.Tensor,
                        end_positions: torch.Tensor) -> torch.Tensor:
        """Compute standard QA loss with cross-entropy."""

        # Ignore indices where position is -1 (padding/invalid)
        start_mask = start_positions != -1
        end_mask = end_positions != -1

        if start_mask.sum() == 0 or end_mask.sum() == 0:
            return torch.tensor(0.0, device=start_logits.device, requires_grad=True)

        start_loss = F.cross_entropy(
            start_logits[start_mask],
            start_positions[start_mask]
        )
        end_loss = F.cross_entropy(
            end_logits[end_mask],
            end_positions[end_mask]
        )

        return (start_loss + end_loss) / 2

    def evaluate(self) -> Dict[str, float]:
        """Evaluate model with comprehensive metrics."""

        self.model.eval()
        self.model.set_inference_mode("adaptive")

        eval_metrics = defaultdict(float)
        num_examples = 0

        all_predictions = []
        all_ground_truths = []
        efficiency_scores = []

        with torch.no_grad():
            for batch in tqdm(self.eval_loader, desc="Evaluating"):

                # Adaptive inference for each example
                for batch_idx in range(batch['input_ids'].size(0)):
                    question_ids = batch['question_input_ids'][batch_idx]
                    context_segments = batch['context_segments'][batch_idx]

                    result = self.model.adaptive_inference(
                        question_input_ids=question_ids,
                        context_segments=context_segments
                    )

                    # Ground truth
                    gt_span = (
                        batch['global_start_positions'][batch_idx].item(),
                        batch['global_end_positions'][batch_idx].item()
                    )

                    # Compute metrics
                    f1 = self.model.halting_policy.compute_f1_score(
                        result.answer_span, gt_span
                    )
                    exact_match = (result.answer_span[0] == gt_span[0] and
                                 result.answer_span[1] == gt_span[1])

                    eval_metrics['f1'] += f1
                    eval_metrics['exact_match'] += float(exact_match)
                    eval_metrics['confidence'] += result.confidence
                    efficiency_scores.append(result.efficiency_score)

                    all_predictions.append(result.answer_span)
                    all_ground_truths.append(gt_span)
                    num_examples += 1

        # Average metrics
        for key in eval_metrics:
            eval_metrics[key] /= max(num_examples, 1)

        eval_metrics['efficiency_score'] = np.mean(efficiency_scores)
        eval_metrics['avg_segments_processed'] = np.mean([len(pred) for pred in all_predictions])  # This needs fixing

        # Combined score (weighted accuracy + efficiency)
        eval_metrics['combined_score'] = (
            eval_metrics['f1'] * 0.7 + eval_metrics['efficiency_score'] * 0.3
        )

        return dict(eval_metrics)

    def _check_stage_transition(self) -> None:
        """Check and perform stage transition from supervised to hybrid."""

        if (self.training_stage == "supervised" and
            self.current_epoch >= self.config.rl_start_epoch and
            self.config.use_rl_training):

            self.training_stage = "hybrid"
            self.model.set_training_mode("rl")

            self.logger.info(f"Transitioning to hybrid SL+RL training at epoch {self.current_epoch}")
            self.logger.info(f"QA optimizer LR: {self.qa_scheduler.get_last_lr()[0]:.2e}")
            if hasattr(self, 'rl_scheduler'):
                self.logger.info(f"RL optimizer LR: {self.rl_scheduler.get_last_lr()[0]:.2e}")

    def _should_early_stop(self) -> bool:
        """Check if early stopping should be triggered."""

        if not self.config.early_stopping_patience:
            return False

        # Check if eval F1 hasn't improved in patience epochs
        recent_f1_scores = self.training_history['eval_f1'][-self.config.early_stopping_patience:]

        if len(recent_f1_scores) < self.config.early_stopping_patience:
            return False

        max_recent = max(recent_f1_scores)
        if max_recent <= self.best_eval_score:
            self.logger.info(f"Early stopping: no improvement in {self.config.early_stopping_patience} epochs")
            return True

        return False

    def save_checkpoint(self, checkpoint_name: str) -> None:
        """Save training checkpoint."""

        checkpoint = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'qa_optimizer_state_dict': self.qa_optimizer.state_dict(),
            'qa_scheduler_state_dict': self.qa_scheduler.state_dict(),
            'training_stage': self.training_stage,
            'best_eval_score': self.best_eval_score,
            'training_history': dict(self.training_history),
            'config': asdict(self.config)
        }

        if hasattr(self, 'rl_optimizer'):
            checkpoint['rl_optimizer_state_dict'] = self.rl_optimizer.state_dict()
        if hasattr(self, 'rl_scheduler'):
            checkpoint['rl_scheduler_state_dict'] = self.rl_scheduler.state_dict()

        checkpoint_path = os.path.join(self.config.output_dir, f"{checkpoint_name}.pt")
        torch.save(checkpoint, checkpoint_path)

        self.logger.info(f"Checkpoint saved: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str) -> None:
        """Load training checkpoint."""

        self.logger.info(f"Loading checkpoint: {checkpoint_path}")

        checkpoint = torch.load(checkpoint_path, map_location='cpu')

        self.current_epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.training_stage = checkpoint['training_stage']
        self.best_eval_score = checkpoint['best_eval_score']
        self.training_history = defaultdict(list, checkpoint['training_history'])

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.qa_optimizer.load_state_dict(checkpoint['qa_optimizer_state_dict'])
        self.qa_scheduler.load_state_dict(checkpoint['qa_scheduler_state_dict'])

        if 'rl_optimizer_state_dict' in checkpoint and hasattr(self, 'rl_optimizer'):
            self.rl_optimizer.load_state_dict(checkpoint['rl_optimizer_state_dict'])
        if 'rl_scheduler_state_dict' in checkpoint and hasattr(self, 'rl_scheduler'):
            self.rl_scheduler.load_state_dict(checkpoint['rl_scheduler_state_dict'])

        self.model.set_training_mode(self.training_stage)

        self.logger.info(f"Resumed from epoch {self.current_epoch}, stage {self.training_stage}")

    def _setup_logger(self) -> logging.Logger:
        """Setup training logger."""

        logger = logging.getLogger("RBSHybridTrainer")
        logger.setLevel(logging.INFO)

        # File handler
        os.makedirs(self.config.output_dir, exist_ok=True)
        file_handler = logging.FileHandler(
            os.path.join(self.config.output_dir, "training.log")
        )
        file_handler.setLevel(logging.INFO)

        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

        return logger

    def _setup_wandb(self) -> bool:
        """Setup Weights & Biases logging if configured."""

        if not self.config.use_wandb:
            return False

        try:
            import wandb

            wandb.init(
                project=self.config.wandb_project,
                name=self.config.run_name,
                config=asdict(self.config),
                dir=self.config.output_dir
            )

            self.logger.info("WandB logging initialized")
            return True

        except ImportError:
            self.logger.warning("WandB not available, skipping wandb logging")
            return False
        except Exception as e:
            self.logger.warning(f"Failed to initialize WandB: {str(e)}")
            return False

    def _log_epoch_metrics(self,
                          epoch: int,
                          train_metrics: Dict[str, float],
                          eval_metrics: Optional[Dict[str, float]] = None) -> None:
        """Log epoch-level metrics."""

        # Update training history
        for key, value in train_metrics.items():
            self.training_history[f'train_{key}'].append(value)

        if eval_metrics:
            for key, value in eval_metrics.items():
                self.training_history[f'eval_{key}'].append(value)

        # Log to console
        log_str = f"Epoch {epoch}: "
        log_str += ", ".join([f"{k}: {v:.4f}" for k, v in train_metrics.items()])

        if eval_metrics:
            log_str += " | Eval: "
            log_str += ", ".join([f"{k}: {v:.4f}" for k, v in eval_metrics.items()])

        self.logger.info(log_str)

        # Log to WandB
        if self.wandb_enabled:
            import wandb

            wandb_log_data = {f"train/{k}": v for k, v in train_metrics.items()}

            if eval_metrics:
                wandb_log_data.update({f"eval/{k}": v for k, v in eval_metrics.items()})

            wandb_log_data.update({
                "epoch": epoch,
                "global_step": self.global_step,
                "learning_rate_qa": self.qa_scheduler.get_last_lr()[0],
                "training_stage": self.training_stage
            })

            if hasattr(self, 'rl_scheduler'):
                wandb_log_data["learning_rate_rl"] = self.rl_scheduler.get_last_lr()[0]

            wandb.log(wandb_log_data)

    def _log_batch_metrics(self, metrics: Dict[str, float], batch_idx: int) -> None:
        """Log batch-level metrics (less frequently)."""

        if self.wandb_enabled and self.global_step % self.config.logging_steps == 0:
            import wandb
            wandb.log({
                f"batch/{k}": v for k, v in metrics.items()
            }, step=self.global_step)

    def _log_eval_metrics(self, epoch: int, eval_metrics: Dict[str, float]) -> None:
        """Log evaluation metrics with emphasis."""

        self.logger.info(f"Evaluation Results - Epoch {epoch}:")
        for key, value in eval_metrics.items():
            self.logger.info(f"  {key}: {value:.4f}")

        if self.wandb_enabled:
            import wandb
            wandb.log({
                f"eval_final/{k}": v for k, v in eval_metrics.items()
            }, step=self.global_step)


class CheckpointManager:
    """Manages checkpoint saving and cleanup."""

    def __init__(self, save_dir: str, save_frequency: int, keep_best: int = 3):
        self.save_dir = save_dir
        self.save_frequency = save_frequency
        self.keep_best = keep_best

    def save_checkpoint(self, checkpoint_data: Dict, name: str) -> str:
        """Save checkpoint and manage cleanup."""

        checkpoint_path = os.path.join(self.save_dir, f"{name}.pt")
        torch.save(checkpoint_data, checkpoint_path)

        # Cleanup old checkpoints
        if name.startswith("epoch-"):
            self._cleanup_old_checkpoints()

        return checkpoint_path

    def _cleanup_old_checkpoints(self) -> None:
        """Remove old epoch checkpoints to save space."""

        checkpoints = []
        for filename in os.listdir(self.save_dir):
            if filename.startswith("epoch-") and filename.endswith(".pt"):
                epoch_num = int(filename.replace("epoch-", "").replace(".pt", ""))
                checkpoints.append((epoch_num, filename))

        # Sort by epoch number
        checkpoints.sort(key=lambda x: x[0])

        # Keep only the most recent ones
        if len(checkpoints) > self.keep_best:
            for epoch_num, filename in checkpoints[:-self.keep_best]:
                os.remove(os.path.join(self.save_dir, filename))
```

### 2. Training Configuration

**File Location**: `src/rbsqa/configs/hybrid_training_config.py`

```python
@dataclass
class RBSTrainingConfig:
    # Core training settings
    output_dir: str = "./outputs/rbs_experiment"
    run_name: str = "rbs-experiment"
    num_epochs: int = 10
    batch_size: int = 8
    learning_rate: float = 5e-5
    weight_decay: float = 0.01
    adam_epsilon: float = 1e-8
    max_grad_norm: float = 1.0

    # RL training settings
    use_rl_training: bool = True
    rl_start_epoch: int = 2
    rl_weight: float = 0.1
    rl_learning_rate: float = 1e-4
    rl_weight_decay: float = 0.01
    rl_batch_size: int = 8
    lambda_cost: float = 0.01
    gamma: float = 0.99
    use_value_baseline: bool = True
    value_weight: float = 0.5

    # Model architecture settings
    memory_num_tokens: int = 16
    num_memory_experts: int = 4
    use_rbs_mode: bool = True
    belief_state_threshold: float = 0.7

    # Data loading settings
    dataloader_num_workers: int = 4
    dataloader_pin_memory: bool = True

    # Training stability settings
    warmup_steps: int = 500
    max_steps: int = -1  # -1 for epoch-based training
    gradient_accumulation_steps: int = 1

    # Evaluation and saving
    eval_frequency: int = 1
    save_frequency: int = 2
    keep_best_checkpoints: int = 3
    logging_steps: int = 50

    # Early stopping
    early_stopping_patience: int = 5
    early_stopping_threshold: float = 0.001

    # Hardware settings
    device: str = "auto"  # "auto", "cpu", "cuda", "cuda:0", etc.
    mixed_precision: bool = True
    dataloader_prefetch_factor: int = 2

    # Logging settings
    use_wandb: bool = False
    wandb_project: str = "rbs-qa"
    log_level: str = "INFO"

    # Reproducibility
    seed: int = 42

    @classmethod
    def from_args(cls, args: argparse.Namespace) -> "RBSTrainingConfig":
        """Create config from command line arguments."""
        config_dict = {}

        for field_name, field_def in cls.__dataclass_fields__.items():
            if hasattr(args, field_name):
                config_dict[field_name] = getattr(args, field_name)
            elif field_def.default is not dataclasses.MISSING:
                config_dict[field_name] = field_def.default

        return cls(**config_dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert config to dictionary."""
        return asdict(self)

    def save(self, save_path: str) -> None:
        """Save config to JSON file."""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, "w") as f:
            json.dump(self.to_dict(), f, indent=2)

    @classmethod
    def load(cls, load_path: str) -> "RBSTrainingConfig":
        """Load config from JSON file."""
        with open(load_path, "r") as f:
            config_dict = json.load(f)
        return cls(**config_dict)


def create_training_script() -> None:
    """Create main training script that uses the hybrid trainer."""

    script_content = '''#!/usr/bin/env python3
"""
RBS-QA Hybrid Training Script

Usage:
    python train_rbs_hybrid.py --config configs/rbs_balanced.yaml
    python train_rbs_hybrid.py --output_dir ./experiment1 --num_epochs 15
"""

import argparse
import logging
import os
import sys
from pathlib import Path

import torch
from transformers import set_seed

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rbsqa.training.hybrid_trainer import RBSHybridTrainer
from rbsqa.configs.hybrid_training_config import RBSTrainingConfig
from rbsqa.data.rbs_dataset import RBSQADataset
from rbsqa.models.rbs_xlnet import RBSXLNetForQA


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="RBS-QA Hybrid Training")

    # Config file
    parser.add_argument("--config", type=str, default=None,
                       help="Path to config file")

    # Override common parameters
    parser.add_argument("--output_dir", type=str, default="./outputs/rbs_experiment",
                       help="Output directory")
    parser.add_argument("--run_name", type=str, default="rbs-experiment",
                       help="Run name for logging")
    parser.add_argument("--num_epochs", type=int, default=None,
                       help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=None,
                       help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=None,
                       help="Learning rate")
    parser.add_argument("--memory_num_tokens", type=int, default=None,
                       help="Number of memory tokens")
    parser.add_argument("--num_memory_experts", type=int, default=None,
                       help="Number of memory experts")

    # Training mode
    parser.add_argument("--use_rl_training", action="store_true", default=None,
                       help="Enable RL training")
    parser.add_argument("--no_rl_training", dest="use_rl_training", action="store_false",
                       help="Disable RL training")

    # Data
    parser.add_argument("--train_file", type=str, required=True,
                       help="Training data file")
    parser.add_argument("--eval_file", type=str, default=None,
                       help="Evaluation data file")

    # Model
    parser.add_argument("--model_name_or_path", type=str, default="xlnet-base-cased",
                       help="Base model name or path")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None,
                       help="Resume from checkpoint")

    # Hardware
    parser.add_argument("--device", type=str, default="auto",
                       help="Device to use")
    parser.add_argument("--mixed_precision", action="store_true", default=True,
                       help="Use mixed precision training")
    parser.add_argument("--no_mixed_precision", dest="mixed_precision", action="store_false",
                       help="Disable mixed precision")

    # Reproducibility
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed")

    # Logging
    parser.add_argument("--use_wandb", action="store_true",
                       help="Enable Weights & Biases logging")
    parser.add_argument("--wandb_project", type=str, default="rbs-qa",
                       help="WandB project name")

    return parser.parse_args()


def main():
    args = parse_args()

    # Load config
    if args.config:
        config = RBSTrainingConfig.load(args.config)
    else:
        config = RBSTrainingConfig()

    # Override config with command line arguments
    for key, value in vars(args).items():
        if value is not None and hasattr(config, key):
            setattr(config, key, value)

    # Setup output directory
    os.makedirs(config.output_dir, exist_ok=True)

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=getattr(logging, config.log_level.upper()),
        handlers=[
            logging.FileHandler(os.path.join(config.output_dir, "training.log")),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)

    # Set seed
    set_seed(config.seed)
    torch.manual_seed(config.seed)

    # Setup device
    if config.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = config.device

    logger.info(f"Using device: {device}")

    # Load datasets
    logger.info(f"Loading training data from: {args.train_file}")
    train_dataset = RBSQADataset.from_file(args.train_file)

    eval_dataset = None
    if args.eval_file:
        logger.info(f"Loading evaluation data from: {args.eval_file}")
        eval_dataset = RBSQADataset.from_file(args.eval_file)

    # Initialize model
    logger.info(f"Initializing RBS-XLNet model with base: {args.model_name_or_path}")
    model = RBSXLNetForQA(
        base_model_name=args.model_name_or_path,
        memory_num_tokens=config.memory_num_tokens,
        num_memory_experts=config.num_memory_experts,
        use_rbs_mode=config.use_rbs_mode
    )

    model = model.to(device)

    # Initialize trainer
    logger.info("Initializing hybrid trainer...")
    trainer = RBSHybridTrainer(
        model=model,
        config=config,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        resume_from_checkpoint=args.resume_from_checkpoint
    )

    # Save config
    config_path = os.path.join(config.output_dir, "training_config.json")
    config.save(config_path)
    logger.info(f"Training config saved to: {config_path}")

    # Start training
    logger.info("Starting hybrid training...")
    training_results = trainer.train()

    logger.info("Training completed successfully!")
    logger.info(f"Final results: {training_results}")


if __name__ == "__main__":
    main()
'''

    # Write training script
    script_path = "scripts/train_rbs_hybrid.py"
    with open(script_path, "w") as f:
        f.write(script_content)

    # Make executable
    os.chmod(script_path, 0o755)
```

### 3. Testing Strategy

**Unit Tests**:
- `test_hybrid_trainer_initialization`
- `test_supervised_training_step`
- `test_hybrid_training_step`
- `test_rl_episode_collection`
- `test_stage_transition_logic`
- `test_checkpoint_save_load`

**Integration Tests**:
- `test_full_hybrid_training_pipeline`
- `test_curriculum_learning_progression`
- `test_evaluation_accuracy_efficiency`
- `test_backward_compatibility_gmm`

**Performance Tests**:
- Training memory usage validation
- Gradient stability across stage transitions
- Convergence speed analysis

### 4. Success Metrics

**Training Stability**:
- QA loss convergence in supervised stage
- RL loss stability in hybrid stage
- No catastrophic forgetting during stage transitions
- Checkpoint loading/resumption works correctly

**Performance Targets**:
- Final F1 score within 2% of GMM baseline
- Efficiency improvement > 30% (fewer segments processed)
- Training completion within expected time
- Memory usage within hardware constraints

### 5. Dependencies

**Required Components**:
- Story 2.1: BeliefStateTracker
- Story 2.2: HaltingPolicyNetwork
- Story 2.3: RBSXLNetForQA
- Existing GMM-XLNet training infrastructure

**External Dependencies**:
- PyTorch >= 2.8.0
- Transformers >= 4.56.2
- tqdm (progress bars)
- wandb (optional, for logging)
- numpy, logging, json, argparse

## Definition of Done

- [ ] All unit tests pass (95%+ coverage)
- [ ] Integration tests demonstrate stable hybrid training
- [ ] Stage transitions work correctly
- [ ] Checkpoint management is robust
- [ ] Evaluation metrics track both accuracy and efficiency
- [ ] Training script is executable and well-documented
- [ ] Configuration system supports all training modes
- [ ] Logging provides sufficient debugging information

## Out of Scope

- Distributed training across multiple GPUs/nodes
- Advanced RL algorithms beyond REINFORCE
- Hyperparameter optimization
- Advanced curriculum learning strategies

## Notes

The hybrid training pipeline is the cornerstone of RBS-QA research. It must balance:

1. **Stability**: Maintain QA accuracy while learning efficiency
2. **Curriculum**: Proper progression from supervised to hybrid learning
3. **Flexibility**: Support various experimental configurations
4. **Robustness**: Handle training interruptions and resume gracefully
5. **Observability**: Provide comprehensive logging and metrics

The two-stage approach ensures the model first learns accurate QA before optimizing for computational efficiency, preventing the RL component from degrading task performance.