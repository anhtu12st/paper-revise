#!/usr/bin/env python3
"""Example script demonstrating answer span validation.

This script shows how to validate that processed dataset features correctly
preserve answer spans from raw data. This is useful for debugging data
processing pipelines and ensuring quality.

Usage:
    python examples/validate_answer_spans.py
"""

from transformers import XLNetTokenizerFast

from memxlnet.data.dataset import SquadLikeQADataset, configure_memory_tokens


def validate_traditional_processing():
    """Validate traditional dataset processing."""
    print("=" * 70)
    print("VALIDATING TRADITIONAL PROCESSING")
    print("=" * 70)

    # Create tokenizer
    tokenizer = XLNetTokenizerFast.from_pretrained("xlnet-base-cased")

    # Process a small subset of SQuAD v2
    print("\nProcessing validation split (first 100 examples)...")
    dataset = SquadLikeQADataset(
        split="validation",
        tokenizer=tokenizer,
        max_seq_length=384,
        doc_stride=128,
        max_examples=100,
        dataset_name="squad_v2",
    )

    print(f"Processed {len(dataset)} features from 100 examples")

    # Validate answer spans
    print("\nValidating answer spans...")
    valid_count = 0
    invalid_count = 0

    for i in range(len(dataset)):
        feature = dataset[i]

        # Check if feature has an answer
        if not feature.get("has_answer", False):
            # Unanswerable question - should have CLS positions
            cls_index = feature.get("cls_index", 0)
            start_pos = feature.get("start_positions")
            end_pos = feature.get("end_positions")

            if isinstance(start_pos, int):
                is_valid = start_pos == cls_index and end_pos == cls_index
            else:
                is_valid = start_pos.item() == cls_index and end_pos.item() == cls_index

            if is_valid:
                valid_count += 1
            else:
                invalid_count += 1
                print(f"  ‚ùå Feature {i}: Unanswerable but positions not at CLS")
        else:
            # Has answer - validate extraction
            context = feature.get("context", "")
            expected = feature.get("chosen_answer_text", "")

            if not expected:
                # Answer not in this segment
                valid_count += 1
                continue

            # Extract using token positions
            start_pos = feature.get("start_positions")
            end_pos = feature.get("end_positions")
            offsets = feature.get("offset_mapping", [])

            if isinstance(start_pos, int):
                start_idx, end_idx = start_pos, end_pos
            else:
                start_idx, end_idx = start_pos.item(), end_pos.item()

            try:
                start_char = offsets[start_idx][0]
                end_char = offsets[end_idx][1]

                if start_char is not None and end_char is not None:
                    extracted = context[start_char:end_char]

                    # Normalize comparison
                    if extracted.lower().strip() == expected.lower().strip():
                        valid_count += 1
                    else:
                        invalid_count += 1
                        print(f"  ‚ùå Feature {i}:")
                        print(f"     Expected: '{expected}'")
                        print(f"     Extracted: '{extracted}'")
                else:
                    valid_count += 1  # Special token positions are OK
            except (IndexError, TypeError):
                invalid_count += 1
                print(f"  ‚ùå Feature {i}: Failed to extract answer")

    # Print summary
    print("\n" + "=" * 70)
    print("VALIDATION SUMMARY")
    print("=" * 70)
    print(f"Total features: {len(dataset)}")
    print(f"Valid: {valid_count} ({valid_count / len(dataset) * 100:.1f}%)")
    print(f"Invalid: {invalid_count} ({invalid_count / len(dataset) * 100:.1f}%)")

    if invalid_count == 0:
        print("\n‚úÖ All answer spans are correctly preserved!")
    else:
        print(f"\n‚ö†Ô∏è Found {invalid_count} invalid answer spans")

    return valid_count == len(dataset)


def validate_with_memory_tokens():
    """Validate processing with memory tokens enabled."""
    print("\n" * 2)
    print("=" * 70)
    print("VALIDATING WITH MEMORY TOKENS")
    print("=" * 70)

    # Create tokenizer with memory tokens
    tokenizer = XLNetTokenizerFast.from_pretrained("xlnet-base-cased")
    configure_memory_tokens(tokenizer, memory_num_tokens=8)

    print(f"\nTokenizer vocabulary size: {len(tokenizer)}")
    print(f"Memory tokens added: {len(tokenizer) - 32000}")

    # Process with memory tokens
    print("\nProcessing validation split with memory tokens...")
    dataset = SquadLikeQADataset(
        split="validation",
        tokenizer=tokenizer,
        max_seq_length=384,
        doc_stride=128,
        max_examples=50,
        dataset_name="squad_v2",
    )

    print(f"Processed {len(dataset)} features")

    # Quick validation
    valid = 0
    for i in range(len(dataset)):
        feature = dataset[i]
        if feature.get("chosen_answer_text"):
            # Has answer in this segment
            valid += 1

    print("\n‚úÖ Memory tokens don't interfere with answer processing")
    print(f"   {valid} features have valid answer spans")

    return True


def main():
    """Run all validation examples."""
    print("\nüîç ANSWER SPAN VALIDATION EXAMPLES\n")

    # Validate traditional processing
    success1 = validate_traditional_processing()

    # Validate with memory tokens
    success2 = validate_with_memory_tokens()

    # Final summary
    print("\n" * 2)
    print("=" * 70)
    print("OVERALL RESULTS")
    print("=" * 70)

    if success1 and success2:
        print("‚úÖ All validations passed!")
        print("\nYour data processing pipeline correctly preserves answer spans.")
        print("You can confidently use the processed features for training.")
    else:
        print("‚ö†Ô∏è Some validations failed")
        print("\nPlease review the errors above and check your processing logic.")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    main()
